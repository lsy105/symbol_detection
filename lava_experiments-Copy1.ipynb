{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299951b3-eb44-4de1-ac0b-802e82febfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cpu\n",
      "init done\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dataset import SpikingDataset\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "#import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from dataset import Dataset, SpikingDataset, RegSpikingDataset, NewDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from Loss import KDLoss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "random.seed(1338)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pyESN import ESN\n",
    "from scipy import interpolate\n",
    "from gen_data import *\n",
    "from tanh import tanh\n",
    "import pandas as pd\n",
    "\n",
    "from Loss import ber_loss\n",
    "from preprocessing import *\n",
    "import lava.lib.dl.bootstrap as bootstrap\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "\n",
    "generating_data = False\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "torch.__version__\n",
    "# The coarse network structure is dicated by the Fashion MNIST dataset. \n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    #device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "time_step = 1e-3\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
    "\n",
    "print(\"init done\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cde483e-ae4a-4541-9019-827cd59c85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.25,\n",
    "                'current_decay' : 1, # this must be 1 to use batchnorm\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 1,\n",
    "                'scale_grad'    : 1,\n",
    "            }\n",
    "        neuron_params_norm = {\n",
    "                **neuron_params,\n",
    "                # 'norm'    : slayer.neuron.norm.MeanOnlyBatchNorm,\n",
    "            }\n",
    "\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                bootstrap.block.cuba.Input(neuron_params, weight=1, bias=0), # enable affine transform at input\n",
    "                bootstrap.block.cuba.Dense(neuron_params_norm, input_size, 32, weight_norm=True, weight_scale=2),\n",
    "                #bootstrap.block.cuba.Dense(neuron_params_norm, 512, 512, weight_norm=True, weight_scale=2),\n",
    "                bootstrap.block.cuba.Affine(neuron_params, 32, 2, weight_norm=True, weight_scale=2),\n",
    "            ])\n",
    "\n",
    "    def forward(self, x, mode):\n",
    "        for block, m in zip(self.blocks, mode):\n",
    "            x = block(x, mode=m)\n",
    "        return x\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        simulation = h.create_group('simulation')\n",
    "        simulation['Ts'] = 1\n",
    "        simulation['tSample'] = self.time_steps\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89360224-dfe9-4fa5-9601-35156d438dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, net_snn, optimizer, scheduler, loss_fn):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        X = X.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        re, imag = net_snn(X)\n",
    "        #re = torch.mean(re, dim=2)\n",
    "        #imag = torch.mean(imag, dim=2)\n",
    "        output = torch.cat((re, imag), axis=1)\n",
    "        \n",
    "        #output = net_snn(X)\n",
    "        #output = torch.mean(output, dim=2)\n",
    "        \n",
    "        loss = loss_fn(output, y) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "          \n",
    "\n",
    "def test(test_loader, net_snn, rc, num_frame = 19):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        #output = net_snn(X)\n",
    "        #output = torch.mean(output, dim=2)\n",
    "        re, imag = net_snn(X)\n",
    "        re = torch.mean(re, dim=2)\n",
    "        imag = torch.mean(imag, dim=2)\n",
    "        re, imag = torch.sign(y[:, 0]), torch.sign(y[:, 1])\n",
    "        output = torch.cat((re, imag), axis=1)\n",
    "    \n",
    "        print(output[10], y[10])\n",
    "        all_output.append(output.cpu().detach().numpy())\n",
    "        labels.append(y.cpu().detach().numpy())\n",
    "        \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    predict_time = rc.time_to_freq(all_output, num_frame, remove_delay=False)\n",
    "    target_time = rc.time_to_freq(labels, num_frame, remove_delay=False)\n",
    "    # print(predict_time, target_time)\n",
    "    return rc.my_new_test(predict_time, target_time)\n",
    "\n",
    "def train(train_loader, test_loader, net_snn, epochs=100, rc=None, num_frame = 19):\n",
    "    optimizer = torch.optim.Adam(net_snn.parameters(), lr=5e-3, betas=(0.9,0.999))\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    #loss_fn = torch.nn.L1Loss()\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    for i in range(epochs):\n",
    "        train_epoch(train_loader, net_snn, optimizer, scheduler, loss_fn)\n",
    "        print(i)\n",
    "        print(test(test_loader, net_snn, rc, num_frame=19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3cf707-3545-4848-b3d1-cb4c50516cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61e06b8-eb27-4eb7-94da-09900e4c6834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "-0.02459430120580924 0.021224323747763597\n",
      "[Epoch  0/100] Train (1520, 2) (1520, 2)\n",
      "0.4205043859649123\n",
      "[Epoch  1/100] Train (1520, 2) (1520, 2)\n",
      "0.4649122807017544\n",
      "[Epoch  2/100] Train (1520, 2) (1520, 2)\n",
      "0.4407894736842105\n",
      "[Epoch  3/100] Train (1520, 2) (1520, 2)\n",
      "0.45394736842105265\n",
      "[Epoch  4/100] Train (1520, 2) (1520, 2)\n",
      "0.45997807017543857\n",
      "[Epoch  5/100] Train (1520, 2) (1520, 2)\n",
      "0.46984649122807015\n",
      "[Epoch  6/100] Train (1520, 2) (1520, 2)\n",
      "0.47478070175438597\n",
      "[Epoch  7/100] Train (1520, 2) (1520, 2)\n",
      "0.48135964912280704\n",
      "[Epoch  8/100] Train (1520, 2) (1520, 2)\n",
      "0.4791666666666667\n",
      "[Epoch  9/100] Train (1520, 2) (1520, 2)\n",
      "0.4682017543859649\n",
      "[Epoch 10/100] Train (1520, 2) (1520, 2)\n",
      "0.43256578947368424\n",
      "[Epoch 11/100] Train (1520, 2) (1520, 2)\n",
      "0.46600877192982454\n",
      "[Epoch 12/100] Train (1520, 2) (1520, 2)\n",
      "0.4654605263157895\n",
      "[Epoch 13/100] Train (1520, 2) (1520, 2)\n",
      "0.4621710526315789\n",
      "[Epoch 14/100] Train (1520, 2) (1520, 2)\n",
      "0.4555921052631579\n",
      "[Epoch 15/100] Train (1520, 2) (1520, 2)\n",
      "0.45668859649122806\n",
      "[Epoch 16/100] Train (1520, 2) (1520, 2)\n",
      "0.45394736842105265\n",
      "[Epoch 17/100] Train (1520, 2) (1520, 2)\n",
      "0.4473684210526316\n",
      "[Epoch 18/100] Train (1520, 2) (1520, 2)\n",
      "0.4468201754385965\n",
      "[Epoch 19/100] Train (1520, 2) (1520, 2)\n",
      "0.4418859649122807\n",
      "[Epoch 20/100] Train (1520, 2) (1520, 2)\n",
      "0.40625\n",
      "[Epoch 21/100] Train (1520, 2) (1520, 2)\n",
      "0.43530701754385964\n",
      "[Epoch 22/100] Train (1520, 2) (1520, 2)\n",
      "0.4413377192982456\n",
      "[Epoch 23/100] Train (1520, 2) (1520, 2)\n",
      "0.44353070175438597\n",
      "[Epoch 24/100] Train (1520, 2) (1520, 2)\n",
      "0.43859649122807015\n",
      "[Epoch 25/100] Train (1520, 2) (1520, 2)\n",
      "0.43859649122807015\n",
      "[Epoch 26/100] Train (1520, 2) (1520, 2)\n",
      "0.43914473684210525\n",
      "[Epoch 27/100] Train (1520, 2) (1520, 2)\n",
      "0.43914473684210525\n",
      "[Epoch 28/100] Train (1520, 2) (1520, 2)\n",
      "0.44024122807017546\n",
      "[Epoch 29/100] Train (1520, 2) (1520, 2)\n",
      "0.43201754385964913\n",
      "[Epoch 30/100] Train (1520, 2) (1520, 2)\n",
      "0.42598684210526316\n",
      "[Epoch 31/100] Train (1520, 2) (1520, 2)\n",
      "0.43256578947368424\n",
      "[Epoch 32/100] Train (1520, 2) (1520, 2)\n",
      "0.4369517543859649\n",
      "[Epoch 33/100] Train (1520, 2) (1520, 2)\n",
      "0.43146929824561403\n",
      "[Epoch 34/100] Train (1520, 2) (1520, 2)\n",
      "0.4418859649122807\n",
      "[Epoch 35/100] Train (1520, 2) (1520, 2)\n",
      "0.4375\n",
      "[Epoch 36/100] Train (1520, 2) (1520, 2)\n",
      "0.4380482456140351\n",
      "[Epoch 37/100] Train (1520, 2) (1520, 2)\n",
      "0.43914473684210525\n",
      "[Epoch 38/100] Train (1520, 2) (1520, 2)\n",
      "0.4380482456140351\n",
      "[Epoch 39/100] Train (1520, 2) (1520, 2)\n",
      "0.4380482456140351\n",
      "[Epoch 40/100] Train (1520, 2) (1520, 2)\n",
      "0.42160087719298245\n",
      "[Epoch 41/100] Train (1520, 2) (1520, 2)\n",
      "0.4298245614035088\n",
      "[Epoch 42/100] Train (1520, 2) (1520, 2)\n",
      "0.4369517543859649\n",
      "[Epoch 43/100] Train (1520, 2) (1520, 2)\n",
      "0.43640350877192985\n",
      "[Epoch 44/100] Train (1520, 2) (1520, 2)\n",
      "0.4342105263157895\n",
      "[Epoch 45/100] Train (1520, 2) (1520, 2)\n",
      "0.4375\n",
      "[Epoch 46/100] Train (1520, 2) (1520, 2)\n",
      "0.43201754385964913\n",
      "[Epoch 47/100] Train (1520, 2) (1520, 2)\n",
      "0.43530701754385964\n",
      "[Epoch 48/100] Train (1520, 2) (1520, 2)\n",
      "0.43585526315789475\n",
      "[Epoch 49/100] Train (1520, 2) (1520, 2)\n",
      "0.4298245614035088\n",
      "[Epoch 50/100] Train (1520, 2) (1520, 2)\n",
      "0.4024122807017544\n",
      "[Epoch 51/100] Train (1520, 2) (1520, 2)\n",
      "0.4243421052631579\n",
      "[Epoch 52/100] Train (1520, 2) (1520, 2)\n",
      "0.43256578947368424\n",
      "[Epoch 53/100] Train (1520, 2) (1520, 2)\n",
      "0.4336622807017544\n",
      "[Epoch 54/100] Train (1520, 2) (1520, 2)\n",
      "0.43256578947368424\n",
      "[Epoch 55/100] Train (1520, 2) (1520, 2)\n",
      "0.4292763157894737\n",
      "[Epoch 56/100] Train (1520, 2) (1520, 2)\n",
      "0.43256578947368424\n",
      "[Epoch 57/100] Train (1520, 2) (1520, 2)\n",
      "0.4336622807017544\n",
      "[Epoch 58/100] Train (1520, 2) (1520, 2)\n",
      "0.4331140350877193\n",
      "[Epoch 59/100] Train (1520, 2) (1520, 2)\n",
      "0.4270833333333333\n",
      "[Epoch 60/100] Train (1520, 2) (1520, 2)\n",
      "0.40131578947368424\n",
      "[Epoch 61/100] Train (1520, 2) (1520, 2)\n",
      "0.4281798245614035\n",
      "[Epoch 62/100] Train (1520, 2) (1520, 2)\n",
      "0.4303728070175439\n",
      "[Epoch 63/100] Train (1520, 2) (1520, 2)\n",
      "0.4292763157894737\n",
      "[Epoch 64/100] Train (1520, 2) (1520, 2)\n",
      "0.4309210526315789\n",
      "[Epoch 65/100] Train (1520, 2) (1520, 2)\n",
      "0.43146929824561403\n",
      "[Epoch 66/100] Train (1520, 2) (1520, 2)\n",
      "0.4331140350877193\n",
      "[Epoch 67/100] Train (1520, 2) (1520, 2)\n",
      "0.4309210526315789\n",
      "[Epoch 68/100] Train (1520, 2) (1520, 2)\n",
      "0.4342105263157895\n",
      "[Epoch 69/100] Train (1520, 2) (1520, 2)\n",
      "0.43201754385964913\n",
      "[Epoch 70/100] Train (1520, 2) (1520, 2)\n",
      "0.40021929824561403\n",
      "[Epoch 71/100] Train (1520, 2) (1520, 2)\n",
      "0.4292763157894737\n",
      "[Epoch 72/100] Train (1520, 2) (1520, 2)\n",
      "0.42872807017543857\n",
      "[Epoch 73/100] Train (1520, 2) (1520, 2)\n",
      "0.43146929824561403\n",
      "[Epoch 74/100] Train (1520, 2) (1520, 2)\n",
      "0.4298245614035088\n",
      "[Epoch 75/100] Train (1520, 2) (1520, 2)\n",
      "0.43201754385964913\n",
      "[Epoch 76/100] Train (1520, 2) (1520, 2)\n",
      "0.43256578947368424\n",
      "[Epoch 77/100] Train (1520, 2) (1520, 2)\n",
      "0.4331140350877193\n",
      "[Epoch 78/100] Train (1520, 2) (1520, 2)\n",
      "0.43201754385964913\n",
      "[Epoch 79/100] Train (1520, 2) (1520, 2)\n",
      "0.43201754385964913\n",
      "[Epoch 80/100] Train (1520, 2) (1520, 2)\n",
      "0.3930921052631579\n",
      "[Epoch 81/100] Train (1520, 2) (1520, 2)\n",
      "0.4309210526315789\n",
      "[Epoch 82/100] Train (1520, 2) (1520, 2)\n",
      "0.4303728070175439\n",
      "[Epoch 83/100] Train (1520, 2) (1520, 2)\n",
      "0.43201754385964913\n",
      "[Epoch 84/100] Train (1520, 2) (1520, 2)\n",
      "0.43146929824561403\n",
      "[Epoch 85/100] Train (1520, 2) (1520, 2)\n",
      "0.4336622807017544\n",
      "[Epoch 86/100] Train (1520, 2) (1520, 2)\n",
      "0.43475877192982454\n",
      "[Epoch 87/100] Train (1520, 2) (1520, 2)\n",
      "0.4331140350877193\n",
      "[Epoch 88/100] Train (1520, 2) (1520, 2)\n",
      "0.43201754385964913\n",
      "[Epoch 89/100] Train (1520, 2) (1520, 2)\n",
      "0.43146929824561403\n",
      "[Epoch 90/100] Train (1520, 2) (1520, 2)\n",
      "0.39144736842105265\n",
      "[Epoch 91/100] Train (1520, 2) (1520, 2)\n",
      "0.4331140350877193\n",
      "[Epoch 92/100] Train (1520, 2) (1520, 2)\n",
      "0.4380482456140351\n",
      "[Epoch 93/100] Train (1520, 2) (1520, 2)\n",
      "0.43256578947368424\n",
      "[Epoch 94/100] Train (1520, 2) (1520, 2)\n",
      "0.4380482456140351\n",
      "[Epoch 95/100] Train (1520, 2) (1520, 2)\n",
      "0.4418859649122807\n",
      "[Epoch 96/100] Train (1520, 2) (1520, 2)\n",
      "0.43969298245614036\n",
      "[Epoch 97/100] Train (1520, 2) (1520, 2)\n",
      "0.43914473684210525\n",
      "[Epoch 98/100] Train (1520, 2) (1520, 2)\n",
      "0.4418859649122807\n",
      "[Epoch 99/100] Train (1520, 2) (1520, 2)\n",
      "0.44024122807017546\n",
      "[Epoch 99/100] Train 13\n",
      "-0.022254050068586183 0.023167212542820045\n",
      "[Epoch  0/100] Train (1520, 2) (1520, 2)\n",
      "0.4144736842105263\n",
      "[Epoch  1/100] Train (1520, 2) (1520, 2)\n",
      "0.4369517543859649\n",
      "[Epoch  2/100] Train (1520, 2) (1520, 2)\n",
      "0.43256578947368424\n",
      "[Epoch  3/100] Train (1520, 2) (1520, 2)\n",
      "0.43969298245614036\n",
      "[Epoch  4/100] Train (1520, 2) (1520, 2)\n",
      "0.4375\n",
      "[Epoch  5/100] Train (1520, 2) (1520, 2)\n",
      "0.4506578947368421\n",
      "[Epoch  6/100] Train (1520, 2) (1520, 2)\n",
      "0.43585526315789475\n",
      "[Epoch  7/100] Train (1520, 2) (1520, 2)\n",
      "0.42269736842105265\n",
      "[Epoch  8/100] Train (1520, 2) (1520, 2)\n",
      "0.42214912280701755\n",
      "[Epoch  9/100] Train (1520, 2) (1520, 2)\n",
      "0.43475877192982454\n",
      "[Epoch 10/100] Train (1520, 2) (1520, 2)\n",
      "0.4830043859649123\n",
      "[Epoch 11/100] Train (1520, 2) (1520, 2)\n",
      "0.4161184210526316\n",
      "[Epoch 12/100] Train (1520, 2) (1520, 2)\n",
      "0.41502192982456143\n",
      "[Epoch 13/100] Train (1520, 2) (1520, 2)\n",
      "0.42489035087719296\n",
      "[Epoch 14/100] Train (1520, 2) (1520, 2)\n",
      "0.41721491228070173\n",
      "[Epoch 15/100] Train (1520, 2) (1520, 2)\n",
      "0.41228070175438597\n",
      "[Epoch 16/100] Train (1520, 2) (1520, 2)\n",
      "0.4106359649122807\n",
      "[Epoch 17/100] Train (1520, 2) (1520, 2)\n",
      "0.4095394736842105\n",
      "[Epoch 18/100] Train (1520, 2) (1520, 2)\n",
      "0.40789473684210525\n",
      "[Epoch 19/100] Train (1520, 2) (1520, 2)\n",
      "0.4057017543859649\n",
      "[Epoch 20/100] Train (1520, 2) (1520, 2)\n",
      "0.42214912280701755\n",
      "[Epoch 21/100] Train (1520, 2) (1520, 2)\n",
      "0.3958333333333333\n",
      "[Epoch 22/100] Train (1520, 2) (1520, 2)\n",
      "0.4024122807017544\n",
      "[Epoch 23/100] Train (1520, 2) (1520, 2)\n",
      "0.3958333333333333\n",
      "[Epoch 24/100] Train (1520, 2) (1520, 2)\n",
      "0.3963815789473684\n",
      "[Epoch 25/100] Train (1520, 2) (1520, 2)\n",
      "0.39528508771929827\n",
      "[Epoch 26/100] Train (1520, 2) (1520, 2)\n",
      "0.3969298245614035\n",
      "[Epoch 27/100] Train (1520, 2) (1520, 2)\n",
      "0.39473684210526316\n",
      "[Epoch 28/100] Train (1520, 2) (1520, 2)\n",
      "0.3881578947368421\n",
      "[Epoch 29/100] Train (1520, 2) (1520, 2)\n",
      "0.38103070175438597\n",
      "[Epoch 30/100] Train (1520, 2) (1520, 2)\n",
      "0.4928728070175439\n",
      "[Epoch 31/100] Train (1520, 2) (1520, 2)\n",
      "0.39144736842105265\n",
      "[Epoch 32/100] Train (1520, 2) (1520, 2)\n",
      "0.39035087719298245\n",
      "[Epoch 33/100] Train (1520, 2) (1520, 2)\n",
      "0.38706140350877194\n",
      "[Epoch 34/100] Train (1520, 2) (1520, 2)\n",
      "0.39035087719298245\n",
      "[Epoch 35/100] Train (1520, 2) (1520, 2)\n",
      "0.3919956140350877\n",
      "[Epoch 36/100] Train (1520, 2) (1520, 2)\n",
      "0.3887061403508772\n",
      "[Epoch 37/100] Train (1520, 2) (1520, 2)\n",
      "0.3821271929824561\n",
      "[Epoch 38/100] Train (1520, 2) (1520, 2)\n",
      "0.38048245614035087\n",
      "[Epoch 39/100] Train (1520, 2) (1520, 2)\n",
      "0.3815789473684211\n",
      "[Epoch 40/100] Train (1520, 2) (1520, 2)\n",
      "0.49725877192982454\n",
      "[Epoch 41/100] Train (1520, 2) (1520, 2)\n",
      "0.3782894736842105\n",
      "[Epoch 42/100] Train (1520, 2) (1520, 2)\n",
      "0.37993421052631576\n",
      "[Epoch 43/100] Train (1520, 2) (1520, 2)\n",
      "0.3782894736842105\n",
      "[Epoch 44/100] Train (1520, 2) (1520, 2)\n",
      "0.3821271929824561\n",
      "[Epoch 45/100] Train (1520, 2) (1520, 2)\n",
      "0.3832236842105263\n",
      "[Epoch 46/100] Train (1520, 2) (1520, 2)\n",
      "0.3826754385964912\n",
      "[Epoch 47/100] Train (1520, 2) (1520, 2)\n",
      "0.3821271929824561\n",
      "[Epoch 48/100] Train (1520, 2) (1520, 2)\n",
      "0.3854166666666667\n",
      "[Epoch 49/100] Train (1520, 2) (1520, 2)\n",
      "0.3673245614035088\n",
      "[Epoch 50/100] Train (1520, 2) (1520, 2)\n",
      "0.4906798245614035\n",
      "[Epoch 51/100] Train (1520, 2) (1520, 2)\n",
      "0.36239035087719296\n",
      "[Epoch 52/100] Train (1520, 2) (1520, 2)\n",
      "0.37335526315789475\n",
      "[Epoch 53/100] Train (1520, 2) (1520, 2)\n",
      "0.3744517543859649\n",
      "[Epoch 54/100] Train (1520, 2) (1520, 2)\n",
      "0.36293859649122806\n",
      "[Epoch 55/100] Train (1520, 2) (1520, 2)\n",
      "0.3684210526315789\n",
      "[Epoch 56/100] Train (1520, 2) (1520, 2)\n",
      "0.37390350877192985\n",
      "[Epoch 57/100] Train (1520, 2) (1520, 2)\n",
      "0.37609649122807015\n",
      "[Epoch 58/100] Train (1520, 2) (1520, 2)\n",
      "0.3826754385964912\n",
      "[Epoch 59/100] Train (1520, 2) (1520, 2)\n",
      "0.35855263157894735\n",
      "[Epoch 60/100] Train (1520, 2) (1520, 2)\n",
      "0.4923245614035088\n",
      "[Epoch 61/100] Train (1520, 2) (1520, 2)\n",
      "0.36951754385964913\n",
      "[Epoch 62/100] Train (1520, 2) (1520, 2)\n",
      "0.3656798245614035\n",
      "[Epoch 63/100] Train (1520, 2) (1520, 2)\n",
      "0.3678728070175439\n",
      "[Epoch 64/100] Train (1520, 2) (1520, 2)\n",
      "0.36403508771929827\n",
      "[Epoch 65/100] Train (1520, 2) (1520, 2)\n",
      "0.37225877192982454\n",
      "[Epoch 66/100] Train (1520, 2) (1520, 2)\n",
      "0.37335526315789475\n",
      "[Epoch 67/100] Train (1520, 2) (1520, 2)\n",
      "0.35855263157894735\n",
      "[Epoch 68/100] Train (1520, 2) (1520, 2)\n",
      "0.36019736842105265\n",
      "[Epoch 69/100] Train (1520, 2) (1520, 2)\n",
      "0.36348684210526316\n",
      "[Epoch 70/100] Train (1520, 2) (1520, 2)\n",
      "0.47368421052631576\n",
      "[Epoch 71/100] Train (1520, 2) (1520, 2)\n",
      "0.37225877192982454\n",
      "[Epoch 72/100] Train (1520, 2) (1520, 2)\n",
      "0.36896929824561403\n",
      "[Epoch 73/100] Train (1520, 2) (1520, 2)\n",
      "0.37006578947368424\n",
      "[Epoch 74/100] Train (1520, 2) (1520, 2)\n",
      "0.3667763157894737\n",
      "[Epoch 75/100] Train (1520, 2) (1520, 2)\n",
      "0.3651315789473684\n",
      "[Epoch 76/100] Train (1520, 2) (1520, 2)\n",
      "0.3678728070175439\n",
      "[Epoch 77/100] Train (1520, 2) (1520, 2)\n",
      "0.36622807017543857\n",
      "[Epoch 78/100] Train (1520, 2) (1520, 2)\n",
      "0.37609649122807015\n",
      "[Epoch 79/100] Train (1520, 2) (1520, 2)\n",
      "0.37664473684210525\n",
      "[Epoch 80/100] Train (1520, 2) (1520, 2)\n",
      "0.4928728070175439\n",
      "[Epoch 81/100] Train (1520, 2) (1520, 2)\n",
      "0.36622807017543857\n",
      "[Epoch 82/100] Train (1520, 2) (1520, 2)\n",
      "0.3645833333333333\n",
      "[Epoch 83/100] Train (1520, 2) (1520, 2)\n",
      "0.3645833333333333\n",
      "[Epoch 84/100] Train (1520, 2) (1520, 2)\n",
      "0.37280701754385964\n",
      "[Epoch 85/100] Train (1520, 2) (1520, 2)\n",
      "0.3717105263157895\n",
      "[Epoch 86/100] Train (1520, 2) (1520, 2)\n",
      "0.3618421052631579\n",
      "[Epoch 87/100] Train (1520, 2) (1520, 2)\n",
      "0.36239035087719296\n",
      "[Epoch 88/100] Train (1520, 2) (1520, 2)\n",
      "0.35910087719298245\n",
      "[Epoch 89/100] Train (1520, 2) (1520, 2)\n",
      "0.3618421052631579\n",
      "[Epoch 90/100] Train (1520, 2) (1520, 2)\n",
      "0.48903508771929827\n",
      "[Epoch 91/100] Train (1520, 2) (1520, 2)\n",
      "0.36951754385964913\n",
      "[Epoch 92/100] Train (1520, 2) (1520, 2)\n",
      "0.35855263157894735\n",
      "[Epoch 93/100] Train (1520, 2) (1520, 2)\n",
      "0.3673245614035088\n",
      "[Epoch 94/100] Train (1520, 2) (1520, 2)\n",
      "0.3673245614035088\n",
      "[Epoch 95/100] Train (1520, 2) (1520, 2)\n",
      "0.3656798245614035\n",
      "[Epoch 96/100] Train (1520, 2) (1520, 2)\n",
      "0.3684210526315789\n",
      "[Epoch 97/100] Train (1520, 2) (1520, 2)\n",
      "0.36896929824561403\n",
      "[Epoch 98/100] Train (1520, 2) (1520, 2)\n",
      "0.3580043859649123\n",
      "[Epoch 99/100] Train (1520, 2) (1520, 2)\n",
      "0.36348684210526316\n",
      "[Epoch 99/100] Train 14\n",
      "-0.02259127610945092 0.021251704540755246\n",
      "[Epoch  0/100] Train (1520, 2) (1520, 2)\n",
      "0.5010964912280702\n",
      "[Epoch  1/100] Train (1520, 2) (1520, 2)\n",
      "0.48739035087719296\n",
      "[Epoch  2/100] Train (1520, 2) (1520, 2)\n",
      "0.46655701754385964\n",
      "[Epoch  3/100] Train (1520, 2) (1520, 2)\n",
      "0.4413377192982456\n",
      "[Epoch  4/100] Train (1520, 2) (1520, 2)\n",
      "0.43859649122807015\n",
      "[Epoch  5/100] Train (1520, 2) (1520, 2)\n",
      "0.43585526315789475\n",
      "[Epoch  6/100] Train (1520, 2) (1520, 2)\n",
      "0.4309210526315789\n",
      "[Epoch  7/100] Train (1520, 2) (1520, 2)\n",
      "0.4331140350877193\n",
      "[Epoch  8/100] Train (1520, 2) (1520, 2)\n",
      "0.4298245614035088\n",
      "[Epoch  9/100] Train (1520, 2) (1520, 2)\n",
      "0.4342105263157895\n",
      "[Epoch 10/100] Train (1520, 2) (1520, 2)\n",
      "0.5016447368421053\n",
      "[Epoch 11/100] Train (1520, 2) (1520, 2)\n",
      "0.4205043859649123\n",
      "[Epoch 12/100] Train (1520, 2) (1520, 2)\n",
      "0.40350877192982454\n",
      "[Epoch 13/100] Train (1520, 2) (1520, 2)\n",
      "0.4029605263157895\n",
      "[Epoch 14/100] Train (1520, 2) (1520, 2)\n",
      "0.3930921052631579\n",
      "[Epoch 15/100] Train (1520, 2) (1520, 2)\n",
      "0.39089912280701755\n",
      "[Epoch 16/100] Train (1520, 2) (1520, 2)\n",
      "0.4029605263157895\n",
      "[Epoch 17/100] Train (1520, 2) (1520, 2)\n",
      "0.3848684210526316\n",
      "[Epoch 18/100] Train (1520, 2) (1520, 2)\n",
      "0.3881578947368421\n",
      "[Epoch 19/100] Train (1520, 2) (1520, 2)\n",
      "0.3881578947368421\n",
      "[Epoch 20/100] Train (1520, 2) (1520, 2)\n",
      "0.5148026315789473\n",
      "[Epoch 21/100] Train (1520, 2) (1520, 2)\n",
      "0.39364035087719296\n",
      "[Epoch 22/100] Train (1520, 2) (1520, 2)\n",
      "0.39144736842105265\n",
      "[Epoch 23/100] Train (1520, 2) (1520, 2)\n",
      "0.38377192982456143\n",
      "[Epoch 24/100] Train (1520, 2) (1520, 2)\n",
      "0.3821271929824561\n",
      "[Epoch 25/100] Train (1520, 2) (1520, 2)\n",
      "0.3793859649122807\n",
      "[Epoch 26/100] Train (1520, 2) (1520, 2)\n",
      "0.3892543859649123\n",
      "[Epoch 27/100] Train (1520, 2) (1520, 2)\n",
      "0.3744517543859649\n",
      "[Epoch 28/100] Train (1520, 2) (1520, 2)\n",
      "0.3744517543859649\n",
      "[Epoch 29/100] Train (1520, 2) (1520, 2)\n",
      "0.3744517543859649\n",
      "[Epoch 30/100] Train (1520, 2) (1520, 2)\n",
      "0.45230263157894735\n",
      "[Epoch 31/100] Train (1520, 2) (1520, 2)\n",
      "0.38048245614035087\n",
      "[Epoch 32/100] Train (1520, 2) (1520, 2)\n",
      "0.37280701754385964\n",
      "[Epoch 33/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 34/100] Train (1520, 2) (1520, 2)\n",
      "0.37225877192982454\n",
      "[Epoch 35/100] Train (1520, 2) (1520, 2)\n",
      "0.37280701754385964\n",
      "[Epoch 36/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 37/100] Train (1520, 2) (1520, 2)\n",
      "0.3826754385964912\n",
      "[Epoch 38/100] Train (1520, 2) (1520, 2)\n",
      "0.3667763157894737\n",
      "[Epoch 39/100] Train (1520, 2) (1520, 2)\n",
      "0.37719298245614036\n",
      "[Epoch 40/100] Train (1520, 2) (1520, 2)\n",
      "0.46875\n",
      "[Epoch 41/100] Train (1520, 2) (1520, 2)\n",
      "0.37225877192982454\n",
      "[Epoch 42/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 43/100] Train (1520, 2) (1520, 2)\n",
      "0.3684210526315789\n",
      "[Epoch 44/100] Train (1520, 2) (1520, 2)\n",
      "0.36896929824561403\n",
      "[Epoch 45/100] Train (1520, 2) (1520, 2)\n",
      "0.37335526315789475\n",
      "[Epoch 46/100] Train (1520, 2) (1520, 2)\n",
      "0.3706140350877193\n",
      "[Epoch 47/100] Train (1520, 2) (1520, 2)\n",
      "0.3717105263157895\n",
      "[Epoch 48/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 49/100] Train (1520, 2) (1520, 2)\n",
      "0.37006578947368424\n",
      "[Epoch 50/100] Train (1520, 2) (1520, 2)\n",
      "0.5213815789473685\n",
      "[Epoch 51/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 52/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 53/100] Train (1520, 2) (1520, 2)\n",
      "0.3706140350877193\n",
      "[Epoch 54/100] Train (1520, 2) (1520, 2)\n",
      "0.3651315789473684\n",
      "[Epoch 55/100] Train (1520, 2) (1520, 2)\n",
      "0.3755482456140351\n",
      "[Epoch 56/100] Train (1520, 2) (1520, 2)\n",
      "0.3678728070175439\n",
      "[Epoch 57/100] Train (1520, 2) (1520, 2)\n",
      "0.37006578947368424\n",
      "[Epoch 58/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 59/100] Train (1520, 2) (1520, 2)\n",
      "0.37006578947368424\n",
      "[Epoch 60/100] Train (1520, 2) (1520, 2)\n",
      "0.49780701754385964\n",
      "[Epoch 61/100] Train (1520, 2) (1520, 2)\n",
      "0.3645833333333333\n",
      "[Epoch 62/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 63/100] Train (1520, 2) (1520, 2)\n",
      "0.38048245614035087\n",
      "[Epoch 64/100] Train (1520, 2) (1520, 2)\n",
      "0.3673245614035088\n",
      "[Epoch 65/100] Train (1520, 2) (1520, 2)\n",
      "0.36348684210526316\n",
      "[Epoch 66/100] Train (1520, 2) (1520, 2)\n",
      "0.3706140350877193\n",
      "[Epoch 67/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 68/100] Train (1520, 2) (1520, 2)\n",
      "0.3667763157894737\n",
      "[Epoch 69/100] Train (1520, 2) (1520, 2)\n",
      "0.37006578947368424\n",
      "[Epoch 70/100] Train (1520, 2) (1520, 2)\n",
      "0.4868421052631579\n",
      "[Epoch 71/100] Train (1520, 2) (1520, 2)\n",
      "0.3678728070175439\n",
      "[Epoch 72/100] Train (1520, 2) (1520, 2)\n",
      "0.3706140350877193\n",
      "[Epoch 73/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 74/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 75/100] Train (1520, 2) (1520, 2)\n",
      "0.37006578947368424\n",
      "[Epoch 76/100] Train (1520, 2) (1520, 2)\n",
      "0.3782894736842105\n",
      "[Epoch 77/100] Train (1520, 2) (1520, 2)\n",
      "0.36951754385964913\n",
      "[Epoch 78/100] Train (1520, 2) (1520, 2)\n",
      "0.3815789473684211\n",
      "[Epoch 79/100] Train (1520, 2) (1520, 2)\n",
      "0.37280701754385964\n",
      "[Epoch 80/100] Train (1520, 2) (1520, 2)\n",
      "0.512609649122807\n",
      "[Epoch 81/100] Train (1520, 2) (1520, 2)\n",
      "0.37006578947368424\n",
      "[Epoch 82/100] Train (1520, 2) (1520, 2)\n",
      "0.36622807017543857\n",
      "[Epoch 83/100] Train (1520, 2) (1520, 2)\n",
      "0.3711622807017544\n",
      "[Epoch 84/100] Train (1520, 2) (1520, 2)\n",
      "0.38103070175438597\n",
      "[Epoch 85/100] Train (1520, 2) (1520, 2)\n",
      "0.38048245614035087\n",
      "[Epoch 86/100] Train (1520, 2) (1520, 2)\n",
      "0.37280701754385964\n",
      "[Epoch 87/100] Train (1520, 2) (1520, 2)\n",
      "0.3684210526315789\n",
      "[Epoch 88/100] Train (1520, 2) (1520, 2)\n",
      "0.37006578947368424\n",
      "[Epoch 89/100] Train (1520, 2) (1520, 2)\n",
      "0.37335526315789475\n",
      "[Epoch 90/100] Train (1520, 2) (1520, 2)\n",
      "0.48135964912280704\n",
      "[Epoch 91/100] Train (1520, 2) (1520, 2)\n",
      "0.3782894736842105\n",
      "[Epoch 92/100] Train (1520, 2) (1520, 2)\n",
      "0.3782894736842105\n",
      "[Epoch 93/100] Train (1520, 2) (1520, 2)\n",
      "0.3782894736842105\n",
      "[Epoch 94/100] Train (1520, 2) (1520, 2)\n",
      "0.37719298245614036\n",
      "[Epoch 95/100] Train (1520, 2) (1520, 2)\n",
      "0.37993421052631576\n",
      "[Epoch 96/100] Train (1520, 2) (1520, 2)\n",
      "0.3793859649122807\n",
      "[Epoch 97/100] Train (1520, 2) (1520, 2)\n",
      "0.37390350877192985\n",
      "[Epoch 98/100] Train (1520, 2) (1520, 2)\n",
      "0.37719298245614036\n",
      "[Epoch 99/100] Train (1520, 2) (1520, 2)\n",
      "0.3744517543859649\n",
      "[Epoch 99/100] Train 16\n",
      "-0.020752603420255623 0.02155366279260016\n",
      "[Epoch  0/100] Train (1520, 2) (1520, 2)\n",
      "0.3892543859649123\n",
      "[Epoch  1/100] Train (1520, 2) (1520, 2)\n",
      "0.4166666666666667\n",
      "[Epoch  2/100] Train (1520, 2) (1520, 2)\n",
      "0.4095394736842105\n",
      "[Epoch  3/100] Train (1520, 2) (1520, 2)\n",
      "0.40789473684210525\n",
      "[Epoch  4/100] Train (1520, 2) (1520, 2)\n",
      "0.4232456140350877\n",
      "[Epoch  5/100] Train (1520, 2) (1520, 2)\n",
      "0.4139254385964912\n",
      "[Epoch  6/100] Train (1520, 2) (1520, 2)\n",
      "0.4276315789473684\n",
      "[Epoch  7/100] Train (1520, 2) (1520, 2)\n",
      "0.4166666666666667\n",
      "[Epoch  8/100] Train (1520, 2) (1520, 2)\n",
      "0.42269736842105265\n",
      "[Epoch  9/100] Train (1520, 2) (1520, 2)\n",
      "0.3848684210526316\n",
      "[Epoch 10/100] Train (1520, 2) (1520, 2)\n",
      "0.43201754385964913\n",
      "[Epoch 11/100] Train (1520, 2) (1520, 2)\n",
      "0.39418859649122806\n",
      "[Epoch 12/100] Train (1520, 2) (1520, 2)\n",
      "0.3958333333333333\n",
      "[Epoch 13/100] Train (1520, 2) (1520, 2)\n",
      "0.4106359649122807\n",
      "[Epoch 14/100] Train (1520, 2) (1520, 2)\n",
      "0.40131578947368424\n",
      "[Epoch 15/100] Train (1520, 2) (1520, 2)\n",
      "0.38377192982456143\n",
      "[Epoch 16/100] Train (1520, 2) (1520, 2)\n",
      "0.4161184210526316\n",
      "[Epoch 17/100] Train (1520, 2) (1520, 2)\n",
      "0.4018640350877193\n",
      "[Epoch 18/100] Train (1520, 2) (1520, 2)\n",
      "0.40515350877192985\n",
      "[Epoch 19/100] Train (1520, 2) (1520, 2)\n",
      "0.4128289473684211\n",
      "[Epoch 20/100] Train (1520, 2) (1520, 2)\n",
      "0.47094298245614036\n",
      "[Epoch 21/100] Train (1520, 2) (1520, 2)\n",
      "0.40515350877192985\n",
      "[Epoch 22/100] Train (1520, 2) (1520, 2)\n",
      "0.41721491228070173\n",
      "[Epoch 23/100] Train (1520, 2) (1520, 2)\n",
      "0.4144736842105263\n",
      "[Epoch 24/100] Train (1520, 2) (1520, 2)\n",
      "0.41173245614035087\n",
      "[Epoch 25/100] Train (1520, 2) (1520, 2)\n",
      "0.41173245614035087\n",
      "[Epoch 26/100] Train (1520, 2) (1520, 2)\n",
      "0.41721491228070173\n",
      "[Epoch 27/100] Train (1520, 2) (1520, 2)\n",
      "0.41831140350877194\n",
      "[Epoch 28/100] Train (1520, 2) (1520, 2)\n",
      "0.40021929824561403\n",
      "[Epoch 29/100] Train (1520, 2) (1520, 2)\n",
      "0.4029605263157895\n",
      "[Epoch 30/100] Train (1520, 2) (1520, 2)\n",
      "0.46381578947368424\n",
      "[Epoch 31/100] Train (1520, 2) (1520, 2)\n",
      "0.40789473684210525\n",
      "[Epoch 32/100] Train (1520, 2) (1520, 2)\n",
      "0.41831140350877194\n",
      "[Epoch 33/100] Train (1520, 2) (1520, 2)\n",
      "0.40021929824561403\n",
      "[Epoch 34/100] Train (1520, 2) (1520, 2)\n",
      "0.40131578947368424\n",
      "[Epoch 35/100] Train (1520, 2) (1520, 2)\n",
      "0.42653508771929827\n",
      "[Epoch 36/100] Train (1520, 2) (1520, 2)\n",
      "0.4166666666666667\n",
      "[Epoch 37/100] Train (1520, 2) (1520, 2)\n",
      "0.41776315789473684\n",
      "[Epoch 38/100] Train (1520, 2) (1520, 2)\n",
      "0.40789473684210525\n",
      "[Epoch 39/100] Train (1520, 2) (1520, 2)\n",
      "0.4106359649122807\n",
      "[Epoch 39/100] Train "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m bootstrap\u001b[38;5;241m.\u001b[39mroutine\u001b[38;5;241m.\u001b[39mScheduler()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     93\u001b[0m         net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     94\u001b[0m         mode \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mmode(epoch, i, net\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Research/RC_wifi/dataset.py:81\u001b[0m, in \u001b[0;36mNewDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# X = slayer.utils.time.replicate(X, 2)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(X, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\u001b[38;5;241m.\u001b[39mfloat(), y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "silent = True\n",
    "method = 'RLS'  # RLS; INV; INV+RLS\n",
    "# N_total_frame = 17\n",
    "N_total_frame = 94\n",
    "N_sync_frame = 4\n",
    "# SNR_list = np.arange(1,20,2)\n",
    "SNR_list = [10]\n",
    "nb_inputs = 6\n",
    "\n",
    "# Dataset selection\n",
    "folder_name = 'data/S2/'  # LOS_Near:S2, LOS_Far:S3, NLOS:S1\n",
    "output_folder = 'data_outputs/S1'\n",
    "\n",
    "if folder_name == 'data/S1/':  # NLOS\n",
    "    delay = 0\n",
    "    packet_num = 21\n",
    "elif folder_name == 'data/S2/':  # LOS_Near\n",
    "    delay = 1\n",
    "    packet_num = 27 # correct\n",
    "elif folder_name == 'data/S3/':  # LOS_Far\n",
    "    delay = 1\n",
    "    packet_num = 22 # 23\n",
    "else:\n",
    "    print(\"Undefined Dataset\")\n",
    "    exit(1)\n",
    "    \n",
    "window_size = 2\n",
    "N_reservoir = 16\n",
    "debug = False\n",
    "\n",
    "ber_record = []\n",
    "dfe_ber_record = []\n",
    "LS_ber_record = []\n",
    "comb_ber_record = []\n",
    "sta_ber_record = []\n",
    "tanh_lut = tanh(\n",
    "    input_bit=8,\n",
    "    dx_bit=8,\n",
    "    slope_fmt=(10, 10),\n",
    "    intercept_fmt=(19, 19),\n",
    "    max=8,\n",
    "    better_lut=True,\n",
    "    verbose=False,\n",
    "    plot=False)\n",
    "\n",
    "SNR = SNR_list[0]\n",
    "\n",
    "overall_mu_ber = 0.0\n",
    "overall_teacher_ber = 0.0\n",
    "overall_base_ber = 0.0\n",
    "overall_nopath_ber = 0.0\n",
    "cnt = 0\n",
    "trained_folder = \"trained\"\n",
    "# except 2\n",
    "packets = [11, 13, 14, 16, 17, 18, 19, 20]  # [11, 13, 14, 16,]\n",
    "for i in packets:\n",
    "    print(i)\n",
    "    rc = RC(silent, method, N_total_frame, N_sync_frame, SNR, delay, window_size, i,\n",
    "            N_reservoir=16,\n",
    "            spectral_radius=0.2,\n",
    "            sparsity=0.4,\n",
    "            noise=1e-6,\n",
    "            lut_activation=False,  # True,\n",
    "            tanh_lut=tanh_lut,\n",
    "            input_scale=25,  #40, #50, # 25,\n",
    "            reservoir_input_scale = 8,  #4,  #5,\n",
    "            show_wout=False,\n",
    "            output_folder= output_folder,\n",
    "            debug=debug,\n",
    "            use_fpga= None,\n",
    "            sock= None,  # usock\n",
    "            addr = None) # addr\n",
    "\n",
    "    train_input, train_label, test_input, test_label = rc.run()\n",
    "    train_input, train_label, test_input, test_label = pre_processing(train_input, train_label)\n",
    "    train_label = 100 * train_label\n",
    "    test_label = 100 * test_label\n",
    "    min_val = np.min(train_input)\n",
    "    max_val = np.max(train_input)\n",
    "    print(min_val, max_val)\n",
    "    train_data = NewDataset(train_input, train_label, sequence_length=nb_inputs, time_step=100, min_val=min_val, max_val=max_val)\n",
    "    test_data =  NewDataset(test_input, test_label, sequence_length=nb_inputs, time_step=100, min_val=min_val, max_val=max_val)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=256, shuffle=False, drop_last=False)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=256, shuffle=False, drop_last=False)\n",
    "    net = Network(4, 2).to(device)\n",
    "    stats = slayer.utils.LearningStats()\n",
    "    scheduler = bootstrap.routine.Scheduler()\n",
    "    epochs = 100\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=5e-3, betas=(0.9,0.999))\n",
    "    scheduler = bootstrap.routine.Scheduler()\n",
    "    for epoch in range(epochs):\n",
    "        for i, (input, label) in enumerate(train_loader, 0):\n",
    "            net.train()\n",
    "            mode = scheduler.mode(epoch, i, net.training)\n",
    "\n",
    "            input = input.to(device)\n",
    "            output = net.forward(input, mode)\n",
    "            rate = torch.mean(output, dim=-1).reshape((input.shape[0], -1))\n",
    "\n",
    "            loss = F.l1_loss(rate, label.to(device))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "\n",
    "        \n",
    "        all_output = []\n",
    "        labels = []\n",
    "        for i, (input, label) in enumerate(test_loader, 0):\n",
    "            net.eval()\n",
    "            mode = scheduler.mode(epoch, i, net.training)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                input = input.to(device)\n",
    "                output = net.forward(input, mode=scheduler.mode(epoch, i, net.training))\n",
    "                rate = torch.mean(output, dim=-1).reshape((input.shape[0], -1))\n",
    "            \n",
    "                all_output.append(rate.cpu().detach().numpy())\n",
    "                labels.append(label.cpu().detach().numpy())\n",
    "                \n",
    "        all_output = np.concatenate(all_output, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "        print(all_output.shape, labels.shape)\n",
    "        predict_time = rc.time_to_freq(all_output, 19, remove_delay=False)\n",
    "        target_time = rc.time_to_freq(labels, 19, remove_delay=False)\n",
    "    # print(predict_time, target_time)\n",
    "        print(rc.my_new_test(predict_time, target_time))  \n",
    "\n",
    "        print(f'\\r[Epoch {epoch:2d}/{epochs}] {stats}', end='')\n",
    "\n",
    "        if mode.base_mode == bootstrap.routine.Mode.SNN:\n",
    "            scheduler.sync_snn_stat(stats.testing)\n",
    "            scheduler.update_snn_stat()\n",
    "\n",
    "        stats.update()\n",
    "        #stats.save(trained_folder + '/')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180aab07-e87e-480f-9648-0b6647f9d260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ee75b-dd21-4aca-af4b-7419f714e596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l:Python",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
