{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138b6027",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cpu\n",
      "init done\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "#import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from dataset import Dataset, SpikingDataset, RegSpikingDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from Loss import KDLoss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "random.seed(1338)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pyESN import ESN\n",
    "from scipy import interpolate\n",
    "from gen_data import *\n",
    "from tanh import tanh\n",
    "from DFRSystem import *\n",
    "import pandas as pd\n",
    "\n",
    "from Loss import ber_loss\n",
    "\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "\n",
    "generating_data = False\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "torch.__version__\n",
    "# The coarse network structure is dicated by the Fashion MNIST dataset. \n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    #device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "time_step = 1e-3\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
    "\n",
    "print(\"init done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe3043c-7ed4-4a5d-b124-5e589297efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(train_input, train_label):\n",
    "    idx_p = 10\n",
    "    begin = 0 # N_total_frame * N_sync_frame\n",
    "    \n",
    "    # label index for data\n",
    "    train_input_df = pd.DataFrame(train_input, columns = ['1','2', '3', '4'])\n",
    "    #train_input_df['L1_idx'] = train_input_df.index % idx_p\n",
    "\n",
    "    # label index for label\n",
    "    train_label_df = pd.DataFrame(train_label, columns = ['L1','L2'])\n",
    "    train_label_df['L1_idx'] = train_label_df.index % idx_p\n",
    "    \n",
    "    # split training and testing data\n",
    "    test_input_df, test_label_df = train_input_df.iloc[75* 80 + 1:, :], train_label_df.iloc[75* 80 + 1:, :]\n",
    "    train_input_df, train_label_df = train_input_df.iloc[:75* 80 + 1, :], train_label_df.iloc[:75* 80 + 1, :]\n",
    "\n",
    "    # group by \n",
    "    #mapping = train_label_df.loc[begin:, :].groupby(by='L1_idx').mean().reset_index().loc[:, ['L1', 'L2', 'L1_idx']] \n",
    "    \n",
    "    #train_input_df = pd.merge(train_input_df, mapping, how='left', on='L1_idx')\n",
    "\n",
    "    #train_input_df = pd.get_dummies(train_input_df, prefix=['L'], columns=['L1_idx'])\n",
    "\n",
    "    train_input_df = train_input_df.loc[begin:, :]\n",
    "\n",
    "\n",
    "    #print(train_input_df.head())\n",
    "    \n",
    "    # testing data\n",
    "    # group by\n",
    "    #test_input_df = test_input_df.merge(mapping, how = 'left', on='L1_idx')\n",
    "\n",
    "    #test_input_df = pd.get_dummies(test_input_df, prefix=['L'], columns=['L1_idx'])\n",
    "\n",
    "    #print(test_input_df.head())\n",
    "\n",
    "    train_input = train_input_df.to_numpy()\n",
    "    test_input = test_input_df.to_numpy()\n",
    "    \n",
    "    train_label = train_label_df.drop(['L1_idx'], axis=1).to_numpy()\n",
    "    test_label = test_label_df.drop(['L1_idx'], axis=1).to_numpy()\n",
    "    \n",
    "    #(train_input.shape)\n",
    "    #print(test_input.shape)\n",
    "    \n",
    "    \n",
    "    return train_input, train_label, test_input, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf8e6108-810c-4eaa-a947-72bef20d573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=1, num_layers=1, batch_first=True, bias=False)\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=32, num_layers=1, batch_first=True, bias=False)\n",
    "        #self.rnn.weight_ih_l0.requires_grad_(False)\n",
    "        #self.rnn.weight_hh_l0.requires_grad_(False)\n",
    "        #self.fc1 = nn.Linear(16, 8)\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "        self.fc2 = nn.Linear(16, output_size)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_x, _ = self.rnn(x)\n",
    "        rnn_x = rnn_x[:, -1, :]\n",
    "        #x = x.flatten(start_dim=1)\n",
    "        #x = torch.cat((x, rnn_x), axis=1)\n",
    "        x = self.fc1(rnn_x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.act(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.act(x)\n",
    "        # x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class CNNNet(nn.Module):\n",
    "    def __init__(self, in_ch, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=in_ch, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * input_size, output_size)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99faab4-fe2e-42c0-b1ab-ad1f8b4a3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rnn(test_loader, model, rc, nb_inputs, num_frame=19):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target.cpu().detach().numpy())\n",
    "        target = target.float().to(device)\n",
    "        input = input.float().to(device)\n",
    "        input1 = input1.float().to(device)\n",
    "        input2 = input2.float().to(device)\n",
    "        \n",
    "        input2 = input2.view(input2.shape[0], nb_inputs, 4)\n",
    "        \n",
    "        output = model(input2).cpu().detach().numpy()\n",
    "        all_output.append(output)\n",
    "        \n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "   \n",
    "    predict_time = rc.time_to_freq(all_output, num_frame, remove_delay=False)\n",
    "    target_time = rc.time_to_freq(labels, num_frame, remove_delay=False)\n",
    "    return rc.my_new_test(predict_time, target_time)\n",
    "\n",
    "def test_cnn(test_loader, model, rc, nb_inputs, num_frame=19):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target.cpu().detach().numpy())\n",
    "        target = target.float().to(device)\n",
    "        input = input.float().to(device)\n",
    "        input1 = input1.float().to(device)\n",
    "        input2 = input2.float().to(device)\n",
    "        \n",
    "        input2 = input2.view(input2.shape[0], nb_inputs, 4)\n",
    "        \n",
    "        output = model(input2).cpu().detach().numpy()\n",
    "        all_output.append(output)\n",
    "        \n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "   \n",
    "    predict_time = rc.time_to_freq(all_output, num_frame, remove_delay=False)\n",
    "    target_time = rc.time_to_freq(labels, num_frame, remove_delay=False)\n",
    "    return rc.my_new_test(predict_time, target_time)\n",
    "\n",
    "\n",
    "def test_mlp(test_loader, model, rc, nb_inputs, num_frame=19):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target.cpu().detach().numpy())\n",
    "        target = target.float().to(device)\n",
    "        input = input.float().to(device)\n",
    "        input1 = input1.float().to(device)\n",
    "        input2 = input2.float().to(device)\n",
    "        \n",
    "        input2 = input2.view(input2.shape[0], nb_inputs * 4)\n",
    "        \n",
    "        output = model(input2).cpu().detach().numpy()\n",
    "        all_output.append(output)\n",
    "        \n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "   \n",
    "    predict_time = rc.time_to_freq(all_output, num_frame, remove_delay=False)\n",
    "    target_time = rc.time_to_freq(labels, num_frame, remove_delay=False)\n",
    "    return rc.my_new_test(predict_time, target_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195106cc-9fba-4bef-9c89-e87a89763d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dnn(trainloader, testloader, test_teacher, net, rc, nb_inputs, lr=2e-3, nb_epochs=10):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    loss_hist = []\n",
    "    best_ber = 10000000000.0\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        loss_hist = []\n",
    "        for x1_local, x2_local, x3_local, y_local in trainloader:\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            x3_local = x3_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # lstm model\n",
    "            x3_local = x3_local.view(x3_local.shape[0], nb_inputs, 4)\n",
    "            output = net(x3_local)\n",
    "            \n",
    "            loss = loss_fn(output, y_local)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist.append(loss.item())\n",
    "            \n",
    "        ber = test_teacher(test_loader, net, rc, nb_inputs, num_frame=19) \n",
    "        if ber < best_ber:\n",
    "            best_ber = ber\n",
    "            #torch.save(net.state_dict(), './teacher.pt')\n",
    "    \n",
    "    return best_ber\n",
    "\n",
    "def train_mlp(trainloader, testloader, test_teacher, net, rc, nb_inputs, lr=2e-3, nb_epochs=10):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    loss_hist = []\n",
    "    best_ber = 10000000000.0\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        loss_hist = []\n",
    "        for x1_local, x2_local, x3_local, y_local in trainloader:\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            x3_local = x3_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # lstm model\n",
    "            x3_local = x3_local.view(x3_local.shape[0], nb_inputs * 4)\n",
    "            output = net(x3_local)\n",
    "            \n",
    "            loss = loss_fn(output, y_local)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist.append(loss.item())\n",
    "            \n",
    "        ber = test_teacher(test_loader, net, rc, nb_inputs, num_frame=19) \n",
    "        if ber < best_ber:\n",
    "            best_ber = ber\n",
    "            #torch.save(net.state_dict(), './teacher.pt')\n",
    "    \n",
    "    return best_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663fb05d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "0.11513157894736842\n",
      "0.11348684210526316\n",
      "0.09923245614035088\n",
      "13\n",
      "0.054276315789473686\n",
      "0.049890350877192985\n",
      "0.05921052631578947\n",
      "14\n",
      "0.049342105263157895\n",
      "0.03673245614035088\n",
      "0.04550438596491228\n",
      "16\n",
      "0.11403508771929824\n",
      "0.12116228070175439\n",
      "0.11403508771929824\n",
      "17\n",
      "0.08333333333333333\n",
      "0.0805921052631579\n",
      "0.0581140350877193\n",
      "18\n",
      "0.015350877192982455\n",
      "0.023574561403508772\n",
      "0.05043859649122807\n",
      "19\n",
      "0.023574561403508772\n",
      "0.027412280701754384\n",
      "0.03070175438596491\n",
      "20\n",
      "0.05866228070175439\n",
      "0.03728070175438596\n",
      "0.044407894736842105\n",
      "rnn ber:  0.06421326754385964\n",
      "cnn ber:  0.06126644736842106\n",
      "mlp ber:  0.06270559210526316\n"
     ]
    }
   ],
   "source": [
    "silent = True\n",
    "method = 'RLS'  # RLS; INV; INV+RLS\n",
    "# N_total_frame = 17\n",
    "N_total_frame = 94\n",
    "N_sync_frame = 4\n",
    "# SNR_list = np.arange(1,20,2)\n",
    "SNR_list = [15]\n",
    "\n",
    "# Dataset selection\n",
    "folder_name = 'data/S2/'  # LOS_Near:S2, LOS_Far:S3, NLOS:S1\n",
    "output_folder = 'data_outputs/S1'\n",
    "\n",
    "if folder_name == 'data/S1/':  # NLOS\n",
    "    delay = 0\n",
    "    packet_num = 21\n",
    "elif folder_name == 'data/S2/':  # LOS_Near\n",
    "    delay = 1\n",
    "    packet_num = 27 # correct\n",
    "elif folder_name == 'data/S3/':  # LOS_Far\n",
    "    delay = 1\n",
    "    packet_num = 22 # 23\n",
    "else:\n",
    "    print(\"Undefined Dataset\")\n",
    "    exit(1)\n",
    "    \n",
    "window_size = 2\n",
    "N_reservoir = 16\n",
    "debug = False\n",
    "\n",
    "ber_record = []\n",
    "dfe_ber_record = []\n",
    "LS_ber_record = []\n",
    "comb_ber_record = []\n",
    "sta_ber_record = []\n",
    "tanh_lut = tanh(\n",
    "    input_bit=8,\n",
    "    dx_bit=8,\n",
    "    slope_fmt=(10, 10),\n",
    "    intercept_fmt=(19, 19),\n",
    "    max=8,\n",
    "    better_lut=True,\n",
    "    verbose=False,\n",
    "    plot=False)\n",
    "\n",
    "SNR = SNR_list[0]\n",
    "\n",
    "overall_rnn_ber = 0.0\n",
    "overall_cnn_ber = 0.0\n",
    "overall_mlp_ber = 0.0\n",
    "cnt = 0\n",
    "\n",
    "# except 2\n",
    "packets = [11, 13, 14, 16, 17, 18, 19, 20]\n",
    "for i in packets:\n",
    "    print(i)\n",
    "    rc = RC(silent, method, N_total_frame, N_sync_frame, SNR, delay, window_size, i,\n",
    "            N_reservoir=16,\n",
    "            spectral_radius=0.2,\n",
    "            sparsity=0.4,\n",
    "            noise=1e-6,\n",
    "            lut_activation=False,  # True,\n",
    "            tanh_lut=tanh_lut,\n",
    "            input_scale=25,  #40, #50, # 25,\n",
    "            reservoir_input_scale = 8,  #4,  #5,\n",
    "            show_wout=False,\n",
    "            output_folder= output_folder,\n",
    "            debug=debug,\n",
    "            use_fpga= None,\n",
    "            sock= None,  # usock\n",
    "            addr = None) # addr\n",
    "\n",
    "    train_input, train_label, test_input, test_label = rc.run()\n",
    "    #RC_test_input = np.load('gt_test_input_1.npy')\n",
    "    #RC_train_input = np.load('gt_train_input_1.npy')\n",
    "    #RC_train_label = np.load('gt_train_label_1.npy')\n",
    " \n",
    "    train_mean = np.mean(train_input)\n",
    "    train_std = np.std(train_input)\n",
    "\n",
    "    train_input = (train_input - train_mean) / train_std\n",
    "    test_input = (test_input - train_mean) / train_std\n",
    "    train_label = 1.0 * train_label\n",
    "    \n",
    "    train_input, train_label, test_input, test_label = pre_processing(train_input, train_label)\n",
    "    \n",
    "    \n",
    "    nb_inputs  = 6\n",
    "    nb_steps = 100\n",
    "    batch_size = 128\n",
    "\n",
    "    import scipy.io\n",
    "\n",
    "    from dataset import Dataset, RegTorchSeasonalitySpikingDataset, RegSpikingDataset, RegTorchSpikingDataset, RegTorchSeasonalityLinearSpikingDataset\n",
    "    train_data = RegTorchSeasonalityLinearSpikingDataset(train_input, train_label, nb_inputs, nb_steps)\n",
    "    test_data = RegTorchSeasonalityLinearSpikingDataset(test_input, test_label, nb_inputs, nb_steps)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "\n",
    "    # RNN model \n",
    "    net_lstm = LSTMNet(4, 2).to(device)\n",
    "    net_cnn = CNNNet(nb_inputs, 4, 2).to(device)\n",
    "    net_mlp = MLPNet(nb_inputs * 4, 2).to(device)\n",
    "    \n",
    "    rnn_best_ber = train_dnn(train_loader, test_loader, test_rnn, net_lstm, rc, nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    cnn_best_ber = train_dnn(train_loader, test_loader, test_cnn, net_cnn, rc, nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    mlp_best_ber = train_mlp(train_loader, test_loader, test_mlp, net_mlp, rc, nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    overall_rnn_ber += rnn_best_ber\n",
    "    overall_cnn_ber += cnn_best_ber\n",
    "    overall_mlp_ber += mlp_best_ber\n",
    "    print(rnn_best_ber)\n",
    "    print(cnn_best_ber)\n",
    "    print(mlp_best_ber)\n",
    "    cnt += 1\n",
    "\n",
    "print(\"rnn ber: \", overall_rnn_ber / len(packets))\n",
    "print(\"cnn ber: \", overall_cnn_ber / len(packets))\n",
    "print(\"mlp ber: \", overall_mlp_ber / len(packets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eebf40-ab07-4dda-9afb-3caa814cefa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8a3e3ae-3aa7-4431-b8df-cf3f7de5e591",
   "metadata": {
    "tags": []
   },
   "source": [
    "rnn ber:  0.06421326754385964\n",
    "cnn ber:  0.06126644736842106\n",
    "mlp ber:  0.06270559210526316\n",
    "\n",
    "0 SNR\n",
    "rnn ber:  0.17763157894736842\n",
    "cnn ber:  0.18277138157894735\n",
    "mlp ber:  0.18290844298245612\n",
    "\n",
    "\n",
    "5 SNR\n",
    "rnn ber:  0.0995751096491228\n",
    "cnn ber:  0.11437774122807018\n",
    "mlp ber:  0.1186951754385965\n",
    "\n",
    "10 SNR\n",
    "rnn ber:  0.06428179824561403\n",
    "cnn ber:  0.07072368421052631\n",
    "mlp ber:  0.08216831140350878\n",
    "\n",
    "rnn ber:  0.06777686403508772\n",
    "cnn ber:  0.07819353070175439\n",
    "mlp ber:  0.0918311403508772\n",
    "\n",
    "\n",
    "15 SNR\n",
    "rnn ber:  0.0414610745614035\n",
    "cnn ber:  0.05975877192982456\n",
    "mlp ber:  0.06400767543859649\n",
    "\n",
    "rnn ber:  0.05921052631578948\n",
    "cnn ber:  0.05777138157894737\n",
    "mlp ber:  0.06928453947368422"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0adbc-050c-4e3a-89d8-4cd3e367c3a9",
   "metadata": {},
   "source": [
    "# power \n",
    "RNN = 43W   time = 0.0002124090003967285   energy = 0.0091335870170593255\n",
    "CNN = 43W   time = 0.000151626079082489    energy = 0.006519921400547027\n",
    "MLP = 43W   time = 0.00010849969625473023  energy = 0.004665486928\n",
    "\n",
    "SNN    energy=0.00189"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l:Python",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
