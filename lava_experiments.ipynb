{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "299951b3-eb44-4de1-ac0b-802e82febfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cpu\n",
      "init done\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dataset import SpikingDataset\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "#import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from dataset import Dataset, SpikingDataset, RegSpikingDataset, NewDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from Loss import KDLoss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "random.seed(1338)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pyESN import ESN\n",
    "from scipy import interpolate\n",
    "from gen_data import *\n",
    "from tanh import tanh\n",
    "import pandas as pd\n",
    "\n",
    "from Loss import ber_loss\n",
    "from preprocessing import *\n",
    "\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "\n",
    "generating_data = False\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "torch.__version__\n",
    "# The coarse network structure is dicated by the Fashion MNIST dataset. \n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    #device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "time_step = 1e-3\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
    "\n",
    "print(\"init done\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cde483e-ae4a-4541-9019-827cd59c85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, med_size, \n",
    "                 input_thres, input_tau_grad, input_scale_grad, \n",
    "                 thres, tau_grad, scale_grad):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "       # neuron_params = {\n",
    "       #         'threshold'     : 0.1,\n",
    "       #         'current_decay' : 1,\n",
    "       #         'voltage_decay' : 0.1,\n",
    "       #         'requires_grad' : True,     \n",
    "       #     }\n",
    "        #neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        neuron_params = {\n",
    "                'threshold'     : input_thres,\n",
    "                'current_decay' : input_tau_grad,\n",
    "                'voltage_decay' : input_scale_grad,\n",
    "                'tau_grad'      : tau_grad,\n",
    "                'scale_grad'    : scale_grad,\n",
    "                'requires_grad' : True,  \n",
    "                # 'graded_spike'  : True,\n",
    "            \n",
    "            }\n",
    "        \n",
    "        neuron_input_params = {\n",
    "                'threshold'     : -10.0,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 1e-1,\n",
    "                'requires_grad' : False,  \n",
    "                # 'graded_spike'  : True,\n",
    "            \n",
    "            }\n",
    "        \n",
    "        neuron_params_2 = {\n",
    "                'threshold'     : 0.05,    # delta unit threshold\n",
    "                'tau_grad'      : 0.5,    # delta unit surrogate gradient relaxation parameter\n",
    "                'scale_grad'    : False,      # delta unit surrogate gradient scale parameter\n",
    "                'requires_grad' : True,   # trainable threshold\n",
    "                'shared_param'  : True,   # layer wise threshold\n",
    "                'period'        : 32,\n",
    "                'decay'         : 0.25,\n",
    "                'graded_spike'  : True,\n",
    "                'norm' : slayer.neuron.norm.MeanOnlyBatchNorm,\n",
    "            \n",
    "        }\n",
    "        sdnn_params = { # sigma-delta neuron parameters\n",
    "                'threshold'     : thres,    # delta unit threshold\n",
    "                'tau_grad'      : tau_grad,    # delta unit surrogate gradient relaxation parameter\n",
    "                'scale_grad'    : scale_grad,      # delta unit surrogate gradient scale parameter\n",
    "                'requires_grad' : True,   # trainable threshold\n",
    "                'shared_param'  : True,   # layer wise threshold\n",
    "                'activation'    : F.relu, # activation function\n",
    "            }\n",
    "        \n",
    "        sdnn_input_params = { # sigma-delta neuron parameters\n",
    "                'threshold'     : input_thres,    # delta unit threshold\n",
    "                'tau_grad'      : input_tau_grad,    # delta unit surrogate gradient relaxation parameter\n",
    "                'scale_grad'    : input_scale_grad,      # delta unit surrogate gradient scale parameter\n",
    "                'requires_grad' : True,   # trainable threshold\n",
    "                'shared_param'  : True,   # layer wise threshold\n",
    "                'activation'    : F.relu, # activation function\n",
    "            }\n",
    "        \n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                #nn.Linear(input_size * 4, 32),\n",
    "                #nn.ReLU(),\n",
    "                #nn.Linear(32, output_size)\n",
    "                #slayer.block.cuba.Input(neuron_params),\n",
    "                # slayer.block.cuba.Recurrent(neuron_params, input_size, 32, weight_norm=True, delay=False, weight_scale=2.0),\n",
    "                #slayer.block.rf.Recurrent(neuron_params_2, input_size, 32, weight_norm=True, delay=False, weight_scale=2.0),\n",
    "                #slayer.block.cuba.Conv(neuron_params,  input_size, 24, 3, padding=1, stride=2, weight_scale=2, weight_norm=True),\n",
    "                #slayer.block.rf.Dense(neuron_params=neuron_params_2, in_neurons=input_size, out_neurons=32, weight_norm=True, delay=True, weight_scale=2.0),\n",
    "                #slayer.block.sigma_delta.Input(sdnn_input_params), \n",
    "                #slayer.block.sigma_delta.Dense(neuron_params=sdnn_params, in_neurons=input_size, out_neurons=med_size, weight_norm=True, delay=False, weight_scale=2.0),\n",
    "                #slayer.block.sigma_delta.Output(sdnn_params, in_neurons=64, out_neurons=32, weight_norm=True, weight_scale=2.0),\n",
    "                #slayer.block.sigma_delta.Dense(neuron_params=sdnn_params, in_neurons=64, out_neurons=128, weight_norm=True, delay=False, weight_scale=2.0),\n",
    "                # slayer.block.cuba.Input(neuron_input_params), \n",
    "                #slayer.block.cuba.Input(neuron_input_params),\n",
    "                #slayer.block.cuba.Recurrent(neuron_recurrent_params, input_size, med_size, weight_norm=True, delay=False, weight_scale=2.0),\n",
    "                slayer.block.cuba.Dense(neuron_params=neuron_params, in_neurons=input_size, out_neurons=med_size, weight_norm=True, delay=True, weight_scale=2.0),\n",
    "                #slayer.block.cuba.Dense(neuron_params=neuron_params, in_neurons=32, out_neurons=64, weight_norm=True, delay=True, weight_scale=2.0),\n",
    "                #slayer.synapse.complex.Dense(64, 32, weight_norm=True),\n",
    "                slayer.synapse.complex.Dense(med_size, 1, weight_norm=True),\n",
    "                # slayer.synapse.complex.Affine(neuron_params, 32, 2, weight_norm=True, weight_scale=2.0),\n",
    "                # slayer.synapse.complex.Dense(neuron_params, input_size, 32, weight_norm=True, delay=True, weight_scale=2.0),\n",
    "                # slayer.block.cuba.Affine(neuron_params, 32, 2, weight_norm=True, weight_scale=2.0),\n",
    "            ])\n",
    "        \n",
    "    \n",
    "    def forward(self, spike):\n",
    "        # spike = torch.flatten(spike, start_dim=1)\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        #spike = spike.flatten(start_dim=1)\n",
    "        #x = x.flatten(start_dim=1)\n",
    "        #spike = torch.cat((x, spike), dim=1)\n",
    "        #real = self.fc_real(spike)\n",
    "        #imag = self.fc_imag(spike)\n",
    "        return spike\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89360224-dfe9-4fa5-9601-35156d438dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, net_snn, optimizer, scheduler, loss_fn):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        X = X.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        re, imag = net_snn(X)\n",
    "        re = torch.mean(re, dim=2)\n",
    "        imag = torch.mean(imag, dim=2)\n",
    "        output = torch.cat((re, imag), axis=1)\n",
    "        \n",
    "        loss = loss_fn(output, y) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "          \n",
    "\n",
    "def test(test_loader, net_snn, rc, num_frame = 19):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        #output = net_snn(X)\n",
    "        #output = torch.mean(output, dim=2)\n",
    "        re, imag = net_snn(X)\n",
    "        re = torch.mean(re, dim=2)\n",
    "        imag = torch.mean(imag, dim=2)\n",
    "        output = torch.cat((re.view(-1, 1), imag.view(-1, 1)), axis=1)\n",
    "    \n",
    "        all_output.append(output.cpu().detach().numpy())\n",
    "        labels.append(y.cpu().detach().numpy())\n",
    "        \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    predict_time = rc.time_to_freq(all_output, num_frame, remove_delay=False)\n",
    "    target_time = rc.time_to_freq(labels, num_frame, remove_delay=False)\n",
    "    # print(predict_time, target_time)\n",
    "    return rc.my_new_test(predict_time, target_time)\n",
    "\n",
    "def train(train_loader, test_loader, net_snn, epochs=100, rc=None, num_frame = 19):\n",
    "    optimizer = torch.optim.Adam(net_snn.parameters(), lr=5e-3, betas=(0.9,0.999))\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    #loss_fn = torch.nn.L1Loss()\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    best_acc = 1e6\n",
    "    for i in range(epochs):\n",
    "        train_epoch(train_loader, net_snn, optimizer, scheduler, loss_fn)\n",
    "        acc = test(test_loader, net_snn, rc, num_frame=19)\n",
    "        best_acc = min(best_acc, acc)\n",
    "        \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61e06b8-eb27-4eb7-94da-09900e4c6834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5290570175438597\n",
      "0.5290570175438597\n",
      "0.5290570175438597\n",
      "0.5290570175438597\n",
      "0.5290570175438597\n",
      "0.5290570175438597\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tau_grad \u001b[38;5;129;01min\u001b[39;00m [i \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m scale_grad \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1e-2\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m]:\n\u001b[0;32m--> 109\u001b[0m         best_acc \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmed_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                     \u001b[49m\u001b[43minput_thres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                     \u001b[49m\u001b[43minput_tau_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                     \u001b[49m\u001b[43minput_scale_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mthres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mtau_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mscale_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28mprint\u001b[39m(best_acc)\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m best_acc \u001b[38;5;241m<\u001b[39m global_best_acc:\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mgrid_search\u001b[0;34m(i_packet, med_size, input_thres, input_tau_grad, input_scale_grad, thres, tau_grad, scale_grad)\u001b[0m\n\u001b[1;32m     89\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mtest_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     91\u001b[0m net_snn \u001b[38;5;241m=\u001b[39m Network(\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m nb_inputs, \u001b[38;5;241m2\u001b[39m, \n\u001b[1;32m     92\u001b[0m                   med_size,\n\u001b[1;32m     93\u001b[0m                   input_thres, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m                   tau_grad,\n\u001b[1;32m     98\u001b[0m                   scale_grad)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_snn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, test_loader, net_snn, epochs, rc, num_frame)\u001b[0m\n\u001b[1;32m     54\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e6\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_snn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     acc \u001b[38;5;241m=\u001b[39m test(test_loader, net_snn, rc, num_frame\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m19\u001b[39m)\n\u001b[1;32m     58\u001b[0m     best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(best_acc, acc)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(train_loader, net_snn, optimizer, scheduler, loss_fn)\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      8\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Research/RC_wifi/dataset.py:48\u001b[0m, in \u001b[0;36mSpikingDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m seq \u001b[38;5;241m=\u001b[39m (seq \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin)\n\u001b[1;32m     47\u001b[0m seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(seq)\n\u001b[0;32m---> 48\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mbernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_labels[end \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     50\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(X, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Research/RC_wifi/encoder.py:43\u001b[0m, in \u001b[0;36mbernoulli\u001b[0;34m(datum, time, dt, device, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (datum \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs must be non-negative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m shape, size \u001b[38;5;241m=\u001b[39m datum\u001b[38;5;241m.\u001b[39mshape, datum\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[0;32m---> 43\u001b[0m datum \u001b[38;5;241m=\u001b[39m \u001b[43mdatum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time \u001b[38;5;241m/\u001b[39m dt)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "silent = True\n",
    "method = 'RLS'  # RLS; INV; INV+RLS\n",
    "# N_total_frame = 17\n",
    "N_total_frame = 94\n",
    "N_sync_frame = 4\n",
    "# SNR_list = np.arange(1,20,2)\n",
    "SNR_list = [10]\n",
    "nb_inputs = 6\n",
    "\n",
    "# Dataset selection\n",
    "folder_name = 'data/S2/'  # LOS_Near:S2, LOS_Far:S3, NLOS:S1\n",
    "output_folder = 'data_outputs/S1'\n",
    "\n",
    "if folder_name == 'data/S1/':  # NLOS\n",
    "    delay = 0\n",
    "    packet_num = 21\n",
    "elif folder_name == 'data/S2/':  # LOS_Near\n",
    "    delay = 1\n",
    "    packet_num = 27 # correct\n",
    "elif folder_name == 'data/S3/':  # LOS_Far\n",
    "    delay = 1\n",
    "    packet_num = 22 # 23\n",
    "else:\n",
    "    print(\"Undefined Dataset\")\n",
    "    exit(1)\n",
    "    \n",
    "window_size = 2\n",
    "N_reservoir = 16\n",
    "debug = False\n",
    "\n",
    "ber_record = []\n",
    "dfe_ber_record = []\n",
    "LS_ber_record = []\n",
    "comb_ber_record = []\n",
    "sta_ber_record = []\n",
    "tanh_lut = tanh(\n",
    "    input_bit=8,\n",
    "    dx_bit=8,\n",
    "    slope_fmt=(10, 10),\n",
    "    intercept_fmt=(19, 19),\n",
    "    max=8,\n",
    "    better_lut=True,\n",
    "    verbose=False,\n",
    "    plot=False)\n",
    "\n",
    "SNR = SNR_list[0]\n",
    "\n",
    "overall_mu_ber = 0.0\n",
    "overall_teacher_ber = 0.0\n",
    "overall_base_ber = 0.0\n",
    "overall_nopath_ber = 0.0\n",
    "cnt = 0\n",
    "packets =[11, 13, 14, 16, 17, 18, 19, 20]  # [11, 13, 14, 16,]\n",
    "# except 2\n",
    "def grid_search(i_packet, \n",
    "                med_size, \n",
    "                input_thres, \n",
    "                input_tau_grad, \n",
    "                input_scale_grad, \n",
    "                thres, \n",
    "                tau_grad, \n",
    "                scale_grad):\n",
    "    rc = RC(silent, method, N_total_frame, N_sync_frame, SNR, delay, window_size, i_packet,\n",
    "            N_reservoir=16,\n",
    "            spectral_radius=0.2,\n",
    "            sparsity=0.4,\n",
    "            noise=1e-6,\n",
    "            lut_activation=False,  # True,\n",
    "            tanh_lut=tanh_lut,\n",
    "            input_scale=25,  #40, #50, # 25,\n",
    "            reservoir_input_scale = 8,  #4,  #5,\n",
    "            show_wout=False,\n",
    "            output_folder= output_folder,\n",
    "            debug=debug,\n",
    "            use_fpga= None,\n",
    "            sock= None,  # usock\n",
    "            addr = None) # addr\n",
    "\n",
    "    train_input, train_label, test_input, test_label = rc.run()\n",
    "    train_input, train_label, test_input, test_label = pre_processing(train_input, train_label)\n",
    "    train_label = 100 * train_label\n",
    "    test_label = 100 * test_label\n",
    "    min_val = np.min(train_input)\n",
    "    max_val = np.max(train_input)\n",
    "   # print(min_val, max_val)\n",
    "    train_data = SpikingDataset(train_input, train_label, sequence_length=nb_inputs, time_step=100, min_val=min_val, max_val=max_val)\n",
    "    test_data =  SpikingDataset(test_input, test_label, sequence_length=nb_inputs, time_step=100, min_val=min_val, max_val=max_val)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=256, shuffle=False, drop_last=False)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=256, shuffle=False, drop_last=False)\n",
    "    \n",
    "    net_snn = Network(4 * nb_inputs, 2, \n",
    "                      med_size,\n",
    "                      input_thres, \n",
    "                      input_tau_grad, \n",
    "                      input_scale_grad, \n",
    "                      thres, \n",
    "                      tau_grad,\n",
    "                      scale_grad).to(device)\n",
    "    return train(train_loader, test_loader, net_snn, epochs=100, rc=rc, num_frame=19)\n",
    "global_best_acc = 1e6  \n",
    "best_params = []\n",
    "for med_size in [16, 32, 64, 128, 256, 512]:\n",
    "    for input_thres in [i * 0.1 for i in range(20)]:\n",
    "        for input_tau_grad in [i * 0.1 for i in range(10)]:\n",
    "            for input_scale_grad in [10, 1, 0.1, 1e-2, 1e-3]:\n",
    "                for thres in [i * 0.1 for i in range(1)]:\n",
    "                    for tau_grad in [i * 0.1 for i in range(10)]:\n",
    "                        for scale_grad in [10, 1, 0.1, 1e-2, 1e-3]:\n",
    "                            best_acc = grid_search(11, \n",
    "                                         med_size, \n",
    "                                         input_thres, \n",
    "                                         input_tau_grad, \n",
    "                                         input_scale_grad, \n",
    "                                         thres, \n",
    "                                         tau_grad, \n",
    "                                         scale_grad)\n",
    "                            print(best_acc)\n",
    "                            if best_acc < global_best_acc:\n",
    "                                global_best_acc = best_acc\n",
    "                                best_params = [\n",
    "                                    med_size,\n",
    "                                    input_thres, \n",
    "                                    input_tau_grad, \n",
    "                                    input_scale_grad, \n",
    "                                    thres, \n",
    "                                    tau_grad,\n",
    "                                    scale_grad,\n",
    "                                ]\n",
    "print(\"best performance\")\n",
    "print(global_best_acc)\n",
    "print(best_params)\n",
    "                                \n",
    "                                \n",
    "                             \n",
    "                            \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180aab07-e87e-480f-9648-0b6647f9d260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ee75b-dd21-4aca-af4b-7419f714e596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l:Python",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
