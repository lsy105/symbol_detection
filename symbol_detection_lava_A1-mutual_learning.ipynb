{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138b6027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cpu\n",
      "init done\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "#import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from dataset import Dataset, SpikingDataset, RegSpikingDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from Loss import KDLoss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "random.seed(1338)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pyESN import ESN\n",
    "from scipy import interpolate\n",
    "from gen_data import *\n",
    "from tanh import tanh\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "torch.__version__\n",
    "# The coarse network structure is dicated by the Fashion MNIST dataset. \n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')\n",
    "    #device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "time_step = 1e-3\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
    "\n",
    "print(\"init done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663fb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "silent = True\n",
    "method = 'RLS'  # RLS; INV; INV+RLS\n",
    "# N_total_frame = 17\n",
    "N_total_frame = 94\n",
    "N_sync_frame = 4\n",
    "# SNR_list = np.arange(1,20,2)\n",
    "SNR_list = [1000]\n",
    "\n",
    "# Dataset selection\n",
    "folder_name = 'data/S2/'  # LOS_Near:S2, LOS_Far:S3, NLOS:S1\n",
    "output_folder = 'data_outputs/S1'\n",
    "\n",
    "if folder_name == 'data/S1/':  # NLOS\n",
    "    delay = 0\n",
    "    packet_num = 21\n",
    "elif folder_name == 'data/S2/':  # LOS_Near\n",
    "    delay = 1\n",
    "    packet_num = 27 # correct\n",
    "elif folder_name == 'data/S3/':  # LOS_Far\n",
    "    delay = 1\n",
    "    packet_num = 22 # 23c\n",
    "else:\n",
    "    print(\"Undefined Dataset\")\n",
    "    exit(1)\n",
    "    \n",
    "window_size = 2\n",
    "N_reservoir = 16\n",
    "debug = False\n",
    "\n",
    "ber_record = []\n",
    "dfe_ber_record = []\n",
    "LS_ber_record = []\n",
    "comb_ber_record = []\n",
    "sta_ber_record = []\n",
    "tanh_lut = tanh(\n",
    "    input_bit=8,\n",
    "    dx_bit=8,\n",
    "    slope_fmt=(10, 10),\n",
    "    intercept_fmt=(19, 19),\n",
    "    max=8,\n",
    "    better_lut=True,\n",
    "    verbose=False,\n",
    "    plot=False)\n",
    "\n",
    "SNR = SNR_list[0]\n",
    "i = 1\n",
    "rc = RC(silent, method, N_total_frame, N_sync_frame, SNR, delay, window_size, i,\n",
    "        N_reservoir=16,\n",
    "        spectral_radius=0.2,\n",
    "        sparsity=0.4,\n",
    "        noise=1e-6,\n",
    "        lut_activation=False,  # True,\n",
    "        tanh_lut=tanh_lut,\n",
    "        input_scale=25,  #40, #50, # 25,\n",
    "        reservoir_input_scale = 8,  #4,  #5,\n",
    "        show_wout=False,\n",
    "        output_folder= output_folder,\n",
    "        debug=debug,\n",
    "        use_fpga= None,\n",
    "        sock= None,  # usock\n",
    "        addr = None) # addr\n",
    "\n",
    "train_input, train_label, test_input, test_label = rc.run()\n",
    "train_mean = np.mean(train_input)\n",
    "train_std = np.std(train_input)\n",
    "\n",
    "train_input = (train_input - train_mean) / train_std\n",
    "test_input = (test_input - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61a8c5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7521, 4)\n",
      "(7521, 4)\n",
      "(7521, 2)\n",
      "(89, 64)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(test_input.shape)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0acdc9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7521\n",
      "(32, 100)\n",
      "(1,)\n",
      "(8, 4)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "nb_inputs  = 8\n",
    "nb_hidden  = 96\n",
    "nb_outputs = 2\n",
    "\n",
    "time_step = 1e-3\n",
    "nb_steps  = 100\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "from dataset import Dataset, SpikingDataset, ARegDataset, RegSpikingDataset, RegTorchBothDataset\n",
    "train_data = RegTorchBothDataset(train_input, train_label, nb_inputs, nb_steps)\n",
    "test_data = RegTorchBothDataset(test_input, train_label, nb_inputs, nb_steps)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "print(len(snn_train_data))\n",
    "print(train_data[0][0].shape)\n",
    "print(train_data[0][1].shape)\n",
    "print(train_data[0][2].shape)\n",
    "print(train_data[0][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "daf4c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "       # neuron_params = {\n",
    "       #         'threshold'     : 0.1,\n",
    "       #         'current_decay' : 1,\n",
    "       #         'voltage_decay' : 0.1,\n",
    "       #         'requires_grad' : True,     \n",
    "       #     }\n",
    "        #neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.00,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 3,\n",
    "                'requires_grad' : True,     \n",
    "            }\n",
    "        \n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                #slayer.block.cuba.Input(neuron_params),\n",
    "                slayer.block.cuba.Recurrent(neuron_params, input_size, 64, weight_norm=True, delay=True),\n",
    "                #slayer.block.cuba.Dense(neuron_params, 32, 64, weight_norm=True, delay=True),\n",
    "                slayer.block.cuba.Dense(neuron_params, 64, 128, weight_norm=True, delay=True),\n",
    "                slayer.block.cuba.Dense(neuron_params, 128, output_size, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, input_size, 64, weight_scale=2, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, 64, 128, weight_scale=2, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, 128, output_size, weight_scale=2, weight_norm=True)\n",
    "                #slayer.block.cuba.Recurrent(cuba_params, 100, 50),\n",
    "                #slayer.block.cuba.KWTA(cuba_params, 50, 50, num_winners=5)\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16 + 1, 8)\n",
    "        self.fc3 = nn.Linear(8, 2)\n",
    "        #self.fc3 = nn.Linear(128, 128)\n",
    "        #self.fc4 = nn.Linear(128, 2)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self,x, x1):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        #x = torch.cat((x, x1), axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x) \n",
    "        x = torch.cat((x, x1), axis=1)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc3(x)\n",
    "        #x = self.fc1(x)\n",
    "        #x = self.act(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = self.act(x)\n",
    "        #x = self.fc3(x)\n",
    "        #x = self.act(x)\n",
    "        #x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3c7df-2316-406f-8c9d-40bcf4a195c3",
   "metadata": {},
   "source": [
    "# DNN network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6ece6e3-71e4-481d-baa6-f5bc0e7c68d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 2)\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1084390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.neuron.current_decay torch.Size([1])\n",
      "blocks.0.neuron.voltage_decay torch.Size([1])\n",
      "blocks.0.delay.delay torch.Size([1])\n",
      "blocks.0.input_synapse.weight_g torch.Size([64, 1, 1, 1, 1])\n",
      "blocks.0.input_synapse.weight_v torch.Size([64, 32, 1, 1, 1])\n",
      "blocks.0.recurrent_synapse.weight_g torch.Size([64, 1, 1, 1, 1])\n",
      "blocks.0.recurrent_synapse.weight_v torch.Size([64, 64, 1, 1, 1])\n",
      "blocks.1.neuron.current_decay torch.Size([1])\n",
      "blocks.1.neuron.voltage_decay torch.Size([1])\n",
      "blocks.1.delay.delay torch.Size([1])\n",
      "blocks.1.synapse.weight_g torch.Size([128, 1, 1, 1, 1])\n",
      "blocks.1.synapse.weight_v torch.Size([128, 64, 1, 1, 1])\n",
      "blocks.2.neuron.current_decay torch.Size([1])\n",
      "blocks.2.neuron.voltage_decay torch.Size([1])\n",
      "blocks.2.synapse.weight_g torch.Size([2, 1, 1, 1, 1])\n",
      "blocks.2.synapse.weight_v torch.Size([2, 128, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "net_snn = Network(nb_inputs * 4, 2).to(device)\n",
    "decoder = Decoder(2 * 100, 2).to(device)\n",
    "\n",
    "net_lstm = LSTMNet(4, 2).to(device)\n",
    "\n",
    "for name, weight in net_snn.named_parameters():\n",
    "    print(name, weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bce4a5c2-38de-466c-ab45-ae78ecfc5307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_loss(teach_output, student_output, alpha, target):\n",
    "    # student loss 1\n",
    "    loss1 = torch.mean((student_output - target)**2)\n",
    "    loss2 = torch.mean((student_output - teach_output)**2)\n",
    "    loss = (1 - alpha) * loss1 + alpha * loss2\n",
    "    return loss\n",
    "\n",
    "def all_snn_model(x1, x2, model, decoder):\n",
    "    output = model(x1)\n",
    "    output = decoder(output, x2)\n",
    "    return output\n",
    "    \n",
    "    \n",
    "\n",
    "def mutual_train(trainloader, testloader, snn_model, decoder, dnn_model, lr=2e-3, nb_epochs=10):\n",
    "    params = list(snn_model.parameters()) + list(decoder.parameters())\n",
    "    optimizer_snn = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999))\n",
    "    optimizer_dnn = torch.optim.Adam(dnn_model.parameters(), lr=lr, betas=(0.9,0.999))\n",
    "    scheduler_snn = torch.optim.lr_scheduler.StepLR(optimizer_snn, step_size=100, gamma=0.1)\n",
    "    scheduler_dnn = torch.optim.lr_scheduler.StepLR(optimizer_dnn, step_size=100, gamma=0.1)\n",
    "    \n",
    "    #loss_fn = slayer.loss.SpikeTime(time_constant=2, filter_order=2, reduction='mean').to(device)\n",
    "    #loss_fn = torch.nn.SmoothL1Loss()\n",
    "    loss_hist = []\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        print(e)\n",
    "        loss_hist = []\n",
    "        for x1_local, x2_local, x3_local, y_local in trainloader:\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            x3_local = x3_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            optimizer_snn.zero_grad()\n",
    "            optimizer_dnn.zero_grad()\n",
    "            snn_output = all_snn_model(x1_local, x2_local, snn_model, decoder)\n",
    "            dnn_output = dnn_model(x3_local)\n",
    "            \n",
    "            snn_loss = mutual_loss(dnn_output.detach(), snn_output, alpha=0.1, target=y_local)\n",
    "            dnn_loss = mutual_loss(snn_output.detach(), dnn_output, alpha=0.1, target=y_local)\n",
    "            snn_loss.backward()\n",
    "            dnn_loss.backward()\n",
    "            optimizer_snn.step()\n",
    "            optimizer_dnn.step()\n",
    "            loss_hist.append(snn_loss.item() + dnn_loss.item())\n",
    "        print(np.mean(loss_hist))\n",
    "        \n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f574380-3878-4cbd-9a38-19da46e5efdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.043825347075175686\n",
      "1\n",
      "0.037636231319284286\n",
      "2\n",
      "0.03553285400414745\n",
      "3\n",
      "0.03439109357689523\n",
      "4\n",
      "0.034266040853808744\n",
      "5\n",
      "0.034082308541825514\n",
      "6\n",
      "0.03326823959379631\n",
      "7\n",
      "0.03291601019564195\n",
      "8\n",
      "0.03099907861462147\n",
      "9\n",
      "0.029383155033860544\n",
      "10\n",
      "0.027876510107258366\n",
      "11\n",
      "0.02620818137025492\n",
      "12\n",
      "0.02474479164367826\n",
      "13\n",
      "0.019524440163414987\n",
      "14\n",
      "0.016941420455715794\n",
      "15\n",
      "0.01590654580119871\n",
      "16\n",
      "0.013085434734189914\n",
      "17\n",
      "0.012196117163620006\n",
      "18\n",
      "0.011239294569457929\n",
      "19\n",
      "0.01031244092253905\n",
      "20\n",
      "0.009854796091962796\n",
      "21\n",
      "0.009428580308834218\n",
      "22\n",
      "0.0087826808513008\n",
      "23\n",
      "0.008212285792608982\n",
      "24\n",
      "0.007253291370974601\n",
      "25\n",
      "0.007342300538597155\n",
      "26\n",
      "0.006702015318481599\n",
      "27\n",
      "0.006520603996034617\n",
      "28\n",
      "0.006182447214950255\n",
      "29\n",
      "0.006652373635068031\n",
      "30\n",
      "0.005603012968726866\n",
      "31\n",
      "0.0050537749677028315\n",
      "32\n",
      "0.005157093721702379\n",
      "33\n",
      "0.005573950290593113\n",
      "34\n",
      "0.005143548276125438\n",
      "35\n",
      "0.005643831910782541\n",
      "36\n",
      "0.005448287727816378\n",
      "37\n",
      "0.005685806107683644\n",
      "38\n",
      "0.005375974968478273\n",
      "39\n",
      "0.005258994118282887\n",
      "40\n",
      "0.005170077965515903\n",
      "41\n",
      "0.004861415624302827\n",
      "42\n",
      "0.004810669041589944\n",
      "43\n",
      "0.0044923642081981996\n",
      "44\n",
      "0.004717083016935326\n",
      "45\n",
      "0.0048652610290692965\n",
      "46\n",
      "0.004294974001822993\n",
      "47\n",
      "0.004356145159779432\n",
      "48\n",
      "0.004334344568078296\n",
      "49\n",
      "0.0043925794889219105\n",
      "50\n",
      "0.004783158946261442\n",
      "51\n",
      "0.004638532894837149\n",
      "52\n",
      "0.004301164049400664\n",
      "53\n",
      "0.004207192643908626\n",
      "54\n",
      "0.004352629985324881\n",
      "55\n",
      "0.003807657953229401\n",
      "56\n",
      "0.003822616514822831\n",
      "57\n",
      "0.0042234355696256\n",
      "58\n",
      "0.004408836825967813\n",
      "59\n",
      "0.0043170509210756144\n",
      "60\n",
      "0.004114561480414754\n",
      "61\n",
      "0.0037647653010997417\n",
      "62\n",
      "0.0037748859470149785\n",
      "63\n",
      "0.0040522373618826395\n",
      "64\n",
      "0.003648189036570237\n",
      "65\n",
      "0.004267032923161099\n",
      "66\n",
      "0.0038332517823141256\n",
      "67\n",
      "0.0038087539963317656\n",
      "68\n",
      "0.003882122997310944\n",
      "69\n",
      "0.004023846067188276\n",
      "70\n",
      "0.0037277434600844696\n",
      "71\n",
      "0.004042290257175563\n",
      "72\n",
      "0.00357646669652826\n",
      "73\n",
      "0.0035949518166135176\n",
      "74\n",
      "0.0044058060270568385\n",
      "75\n",
      "0.003990615420122394\n",
      "76\n",
      "0.003813680773896907\n",
      "77\n",
      "0.003412724111250886\n",
      "78\n",
      "0.003574803287076559\n",
      "79\n",
      "0.004170867544648206\n",
      "80\n",
      "0.004124337659312128\n",
      "81\n",
      "0.003934888841023953\n",
      "82\n",
      "0.003808151085819591\n",
      "83\n",
      "0.0038284527244425172\n",
      "84\n",
      "0.003594140820866214\n",
      "85\n",
      "0.0036490561003536303\n",
      "86\n",
      "0.003983840491115589\n",
      "87\n",
      "0.0039265612236488515\n",
      "88\n",
      "0.004461814449622564\n",
      "89\n",
      "0.004002377763785169\n",
      "90\n",
      "0.0038981041565174395\n",
      "91\n",
      "0.004315612870722346\n",
      "92\n",
      "0.0042195107523446615\n",
      "93\n",
      "0.003733952921150268\n",
      "94\n",
      "0.0033791389790367556\n",
      "95\n",
      "0.0034420355452496904\n",
      "96\n",
      "0.00374859811144218\n",
      "97\n",
      "0.00367244822910387\n",
      "98\n",
      "0.003500836217426695\n",
      "99\n",
      "0.0031113029515343104\n"
     ]
    }
   ],
   "source": [
    "mutual_train(train_loader, train_loader, net_snn, decoder, net_lstm, lr=1e-3, nb_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "808d7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_snn.export_hdf5('./net_snn.net')\n",
    "torch.save(net_dnn.state_dict(), './net_dnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64267916-330b-4913-98a7-3c6dcc5ef460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77/3015334240.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).float()\n",
      "/tmp/ipykernel_77/3015334240.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).float()\n",
      "/tmp/ipykernel_77/3015334240.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input1 = torch.tensor(input1).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2])\n",
      "torch.Size([128, 2])\n",
      "torch.Size([128, 2])\n",
      "torch.Size([128, 2])\n",
      "torch.Size([128, 2])\n",
      "torch.Size([128, 2])\n",
      "torch.Size([128, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     12\u001b[0m input1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input1)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 14\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet_snn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m net_dnn(output, input1)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, spike)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, spike):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 37\u001b[0m         spike \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spike\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/lava/lib/dl/slayer/block/base.py:1149\u001b[0m, in \u001b[0;36mAbstractRecurrent.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     dendrite \u001b[38;5;241m=\u001b[39m z[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, time:time \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1148\u001b[0m     feedback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_synapse(spike\u001b[38;5;241m.\u001b[39mreshape(dendrite\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m-> 1149\u001b[0m     spike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneuron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdendrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeedback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m     x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, time:time \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m spike\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspike_state \u001b[38;5;241m=\u001b[39m spike\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mreshape(z\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/lava/lib/dl/slayer/neuron/cuba.py:433\u001b[0m, in \u001b[0;36mNeuron.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;124;03m\"\"\"Computes the full response of the neuron instance to an input.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    The input shape must match with the neuron shape. For the first time,\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    the neuron shape is determined from the input automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m     _, voltage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspike(voltage)\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/lava/lib/dl/slayer/neuron/cuba.py:362\u001b[0m, in \u001b[0;36mNeuron.dynamics\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     current \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(current)\n\u001b[0;32m--> 362\u001b[0m voltage \u001b[38;5;241m=\u001b[39m \u001b[43mleaky_integrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# bias can be enabled by adding it here\u001b[39;49;00m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoltage_decay\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoltage_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersistent_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;66;03m# The state at last time step needs to be cloned. Otherwise,\u001b[39;00m\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;66;03m# the previous result will be affected since\u001b[39;00m\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# it will use same memory space.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/lava/lib/dl/slayer/neuron/dynamics/leaky_integrator.py:93\u001b[0m, in \u001b[0;36mdynamics\u001b[0;34m(input, decay, state, w_scale, threshold, debug)\u001b[0m\n\u001b[1;32m     90\u001b[0m     state \u001b[38;5;241m=\u001b[39m state \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_LIDynamics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     output \u001b[38;5;241m=\u001b[39m Accelerated\u001b[38;5;241m.\u001b[39mleaky_integrator\u001b[38;5;241m.\u001b[39mdynamics(\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mcontiguous(), decay\u001b[38;5;241m.\u001b[39mcontiguous(), state\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m     97\u001b[0m         threshold, w_scale\n\u001b[1;32m     98\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/lava/lib/dl/slayer/neuron/dynamics/leaky_integrator.py:129\u001b[0m, in \u001b[0;36m_LIDynamics.forward\u001b[0;34m(ctx, input, decay, state, threshold, w_scale)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, \u001b[38;5;28minput\u001b[39m, decay, state, threshold, w_scale):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124;03m\"\"\" \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_li_dynamics_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _LIDynamics\u001b[38;5;241m.\u001b[39mDEBUG \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m         _output, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m Accelerated\u001b[38;5;241m.\u001b[39mleaky_integrator\u001b[38;5;241m.\u001b[39mfwd(\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;28minput\u001b[39m, decay, state, threshold, w_scale\n\u001b[1;32m    137\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/lava/lib/dl/slayer/neuron/dynamics/leaky_integrator.py:215\u001b[0m, in \u001b[0;36m_li_dynamics_fwd\u001b[0;34m(input, decay, state, threshold, w_scale, dtype)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m\"\"\" \"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m output_old \u001b[38;5;241m=\u001b[39m (state \u001b[38;5;241m*\u001b[39m w_scale)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(dtype)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 215\u001b[0m decay_int \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m12\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[43mdecay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(dtype)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    216\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m threshold \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m w_scale\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "error = 0.0\n",
    "inputs = []\n",
    "labels = []\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "all_loss =[]\n",
    "test_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "for input, input1, target in test_loader:\n",
    "    inputs.append(input)\n",
    "    labels.append(target)\n",
    "    target = torch.tensor(target).float()\n",
    "    input = torch.tensor(input).float()\n",
    "    input1 = torch.tensor(input1).float()\n",
    "    \n",
    "    output = net_snn(input)\n",
    "    output = net_dnn(output, input1)\n",
    "    print(output.shape)\n",
    "    #print(output, target)\n",
    "    loss_val = loss_fn(output, target)\n",
    "    all_loss.append(loss_val.item())\n",
    "print(np.mean(all_loss))\n",
    "inputs = np.array(inputs)\n",
    "labels = np.array(labels)\n",
    "#inputs = inputs.reshape((inputs.shape[0], -1))\n",
    "print(inputs.shape)\n",
    "rc.my_test()\n",
    "np.save('input_snn.npy', inputs)\n",
    "np.save('label_snn.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7acff8c-50b2-4bd2-93f8-b3106dddedbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_788/494841858.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).float()\n",
      "/tmp/ipykernel_788/494841858.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).float()\n",
      "/tmp/ipykernel_788/494841858.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input1 = torch.tensor(input1).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17778558052434457\n"
     ]
    }
   ],
   "source": [
    "all_output = []\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "for input, input1, _, target in test_loader:\n",
    "    inputs.append(input)\n",
    "    labels.append(target)\n",
    "    target = torch.tensor(target).float()\n",
    "    input = torch.tensor(input).float()\n",
    "    input1 = torch.tensor(input1).float()\n",
    "    \n",
    "    output = net_snn(input)\n",
    "    output = decoder(output, input1).cpu().detach().numpy()\n",
    "    all_output.append(output)\n",
    "    \n",
    "all_output = np.concatenate(all_output, axis=0)\n",
    "ber = rc.my_test(all_output)\n",
    "print(ber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ee219a-952a-4395-b2a9-db85d18f17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9., 9.,  ..., 9., 9., 9.],\n",
      "        [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "        [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "        ...,\n",
      "        [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "        [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "        [9., 9., 9.,  ..., 9., 9., 9.]])\n"
     ]
    }
   ],
   "source": [
    "A1 = 8 * torch.ones((128, 8))\n",
    "A2 = torch.ones((128, 1))\n",
    "print(A1 + A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eebf40-ab07-4dda-9afb-3caa814cefa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l:Python",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
