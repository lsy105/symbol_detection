{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138b6027",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiya/anaconda3/envs/lava_test/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/shiya/anaconda3/envs/lava_test/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cpu\n",
      "init done\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "#import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from dataset import Dataset, SpikingDataset, RegSpikingDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from Loss import KDLoss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "random.seed(1338)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pyESN import ESN\n",
    "from scipy import interpolate\n",
    "from gen_data import *\n",
    "from tanh import tanh\n",
    "from DFRSystem import *\n",
    "import pandas as pd\n",
    "\n",
    "from Loss import ber_loss\n",
    "\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "\n",
    "generating_data = False\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "torch.__version__\n",
    "# The coarse network structure is dicated by the Fashion MNIST dataset. \n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    #device = torch.device('cuda:0')\n",
    "    #device = torch.device(\"cuda\") \n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "time_step = 1e-3\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
    "\n",
    "print(\"init done\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe3043c-7ed4-4a5d-b124-5e589297efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(train_input, train_label):\n",
    "    idx_p = 10\n",
    "    begin = 0 # N_total_frame * N_sync_frame\n",
    "    \n",
    "    # label index for data\n",
    "    train_input_df = pd.DataFrame(train_input, columns = ['1','2', '3', '4'])\n",
    "    #train_input_df['L1_idx'] = train_input_df.index % idx_p\n",
    "\n",
    "    # label index for label\n",
    "    train_label_df = pd.DataFrame(train_label, columns = ['L1','L2'])\n",
    "    train_label_df['L1_idx'] = train_label_df.index % idx_p\n",
    "    \n",
    "    # split training and testing data\n",
    "    test_input_df, test_label_df = train_input_df.iloc[75* 80 + 1:, :], train_label_df.iloc[75* 80 + 1:, :]\n",
    "    train_input_df, train_label_df = train_input_df.iloc[:75* 80 + 1, :], train_label_df.iloc[:75* 80 + 1, :]\n",
    "\n",
    "    # group by \n",
    "    #mapping = train_label_df.loc[begin:, :].groupby(by='L1_idx').mean().reset_index().loc[:, ['L1', 'L2', 'L1_idx']] \n",
    "    \n",
    "    #train_input_df = pd.merge(train_input_df, mapping, how='left', on='L1_idx')\n",
    "\n",
    "    #train_input_df = pd.get_dummies(train_input_df, prefix=['L'], columns=['L1_idx'])\n",
    "\n",
    "    train_input_df = train_input_df.loc[begin:, :]\n",
    "\n",
    "\n",
    "    #print(train_input_df.head())\n",
    "    \n",
    "    # testing data\n",
    "    # group by\n",
    "    #test_input_df = test_input_df.merge(mapping, how = 'left', on='L1_idx')\n",
    "\n",
    "    #test_input_df = pd.get_dummies(test_input_df, prefix=['L'], columns=['L1_idx'])\n",
    "\n",
    "    #print(test_input_df.head())\n",
    "\n",
    "    train_input = train_input_df.to_numpy()\n",
    "    test_input = test_input_df.to_numpy()\n",
    "    \n",
    "    train_label = train_label_df.drop(['L1_idx'], axis=1).to_numpy()\n",
    "    test_label = test_label_df.drop(['L1_idx'], axis=1).to_numpy()\n",
    "    \n",
    "    #(train_input.shape)\n",
    "    #print(test_input.shape)\n",
    "    \n",
    "    \n",
    "    return train_input, train_label, test_input, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf8e6108-810c-4eaa-a947-72bef20d573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "       # neuron_params = {\n",
    "       #         'threshold'     : 0.1,\n",
    "       #         'current_decay' : 1,\n",
    "       #         'voltage_decay' : 0.1,\n",
    "       #         'requires_grad' : True,     \n",
    "       #     }\n",
    "        #neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.2,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 3,\n",
    "                'requires_grad' : True,   \n",
    "            \n",
    "            }\n",
    "        \n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                #slayer.block.cuba.Input(neuron_params),\n",
    "                slayer.block.cuba.Recurrent(neuron_params, input_size, 8, weight_norm=True, delay=True, weight_scale=1.0),\n",
    "                #slayer.block.cuba.Dense(neuron_params, input_size, 8, weight_norm=True, delay=True, weight_scale=1.0),\n",
    "                slayer.block.cuba.Affine(neuron_params, 8, 1, weight_norm=True),\n",
    "                #slayer.block.cuba.Dense(neuron_params, 8, 1, weight_norm=True, delay=True),\n",
    "                #slayer.block.cuba.Dense(neuron_params, 128, output_size, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, input_size, 64, weight_scale=2, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, 64, 128, weight_scale=2, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, 128, output_size, weight_scale=2, weight_norm=True)\n",
    "                #slayer.block.cuba.Recurrent(cuba_params, 100, 50),\n",
    "                #slayer.block.cuba.KWTA(cuba_params, 50, 50, num_winners=5)\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))\n",
    "\n",
    "class DNNNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        #self.fc1 = nn.Linear(input_size, 2)\n",
    "        self.fc1 = nn.Linear(100, 8)\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=6, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        # self.conv2 = torch.nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc2 = nn.Linear(16 * 4 + 8, 2)\n",
    "        #self.act = nn.Tanh()\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self,x, x1, x2):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x2 = x2.view(x2.shape[0], 6, 4)\n",
    "        x2 = self.conv1(x2)\n",
    "        x2 = self.act(x2)\n",
    "        #x2 = self.conv2(x2)\n",
    "        #x2 = self.act(x2)\n",
    "        x2 = x2.flatten(start_dim=1)\n",
    "        x = torch.cat((x, x2), dim=1)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.act(x)\n",
    "        #x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99faab4-fe2e-42c0-b1ab-ad1f8b4a3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, net_snn, net_dnn, rc, num_frame = 19):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target.cpu().clone().detach().numpy())\n",
    "        #target = torch.tensor(target).float()\n",
    "        #input = torch.tensor(input).float()\n",
    "        #input1 = torch.tensor(input1).float()\n",
    "        #input2 = torch.tensor(input2).float\n",
    "        \n",
    "        target = target.float().to(device)\n",
    "        input = input.float().to(device)\n",
    "        input1 = input1.float().to(device)\n",
    "        input2 = input2.float().to(device)\n",
    "    \n",
    "        output = net_snn(input)\n",
    "        \n",
    "        output = net_dnn(output, input1, input2).cpu().clone().detach().numpy()\n",
    "        all_output.append(output)\n",
    "        \n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "   \n",
    "    predict_time = rc.time_to_freq(all_output, num_frame, remove_delay=False)\n",
    "    target_time = rc.time_to_freq(labels, num_frame, remove_delay=False)\n",
    "    return rc.my_new_test(predict_time, target_time)\n",
    "\n",
    "\n",
    "def test_teacher(test_loader, model, rc, nb_inputs, num_frame=19):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target.cpu().clone().detach().numpy())\n",
    "        #target = torch.tensor(target).float()\n",
    "        #input = torch.tensor(input).float()\n",
    "        #input1 = torch.tensor(input1).float()\n",
    "        #input2 = torch.tensor(input2).float()\n",
    "        target = target.float().to(device)\n",
    "        input = input.float().to(device)\n",
    "        input1 = input1.float().to(device)\n",
    "        input2 = input2.float().to(device)\n",
    "        \n",
    "        input2 = input2.view(input2.shape[0], nb_inputs, 4)\n",
    "        prev_out = 0.1 * torch.zeros(input2.size(0), model.n_hidden).to(device)\n",
    "        output = model(input2, prev_out)[0].cpu().clone().detach().numpy()\n",
    "        all_output.append(output)\n",
    "        \n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "   \n",
    "    #print(\"testing teacher MSE: \", torch.nn.MSELoss()(torch.tensor(all_output), torch.tensor(labels)))\n",
    "    #print(\"testing teacher MSE 0: \", torch.nn.MSELoss()(torch.tensor(all_output[:, 0]), torch.tensor(labels[:, 0])))\n",
    "    #print(\"testing teacher MSE 1: \", torch.nn.MSELoss()(torch.tensor(all_output[:, 1]), torch.tensor(labels[:, 1])))\n",
    "    #print(all_output.shape, labels.shape)\n",
    "    #print(\"percent teacher loss: \", np.mean(np.abs(all_output - labels) / (np.abs(labels) + 1e-6)))\n",
    "    predict_time = rc.time_to_freq(all_output, num_frame, remove_delay=False)\n",
    "    target_time = rc.time_to_freq(labels, num_frame, remove_delay=False)\n",
    "    return rc.my_new_test(predict_time, target_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195106cc-9fba-4bef-9c89-e87a89763d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dnn(trainloader, testloader, net, rc, nb_inputs, lr=2e-3, nb_epochs=10):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "    \n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    loss_hist = []\n",
    "    best_ber = 10000000000.0\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        loss_hist = []\n",
    "        for x1_local, x2_local, x3_local, y_local in trainloader:\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            x3_local = x3_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # lstm model\n",
    "            x3_local = x3_local.view(x3_local.shape[0], nb_inputs, 4)\n",
    "            prev_out = torch.zeros(x3_local.size(0), net.n_hidden).to(device)\n",
    "            output = net(x3_local, prev_out)[0]\n",
    "            \n",
    "            loss = loss_fn(output, y_local)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist.append(loss.item())\n",
    "        \n",
    "        ber = test_teacher(test_loader, net, rc, nb_inputs, num_frame=19) \n",
    "        if ber < best_ber:\n",
    "            best_ber = ber\n",
    "            torch.save(net.state_dict(), './teacher.pt')\n",
    "    return best_ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d365927f-38ef-4ec0-bc13-068701186303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_loss(teacher_output, student_output, alpha, target, mode=True):\n",
    "    # student loss 1\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    \n",
    "    #compare \n",
    "    loss_t = loss_fn(teacher_output, target)\n",
    "    loss_s = loss_fn(student_output, target)\n",
    "    \n",
    "    loss1 = loss_fn(student_output, target)\n",
    "    loss2 = loss_fn(student_output, teacher_output)\n",
    "    if mode:\n",
    "        if loss_t >= loss_s:\n",
    "            return loss1\n",
    "        else:\n",
    "            return loss1 + alpha * loss2 #loss1 + alpha * loss2\n",
    "    else:\n",
    "        return loss1 + alpha * loss2\n",
    "    \n",
    "    \n",
    "def mutual_loss1(teacher_output, student_output, alpha, target, mode=True):\n",
    "    # student loss 1\n",
    "    loss_fn = torch.nn.L1Loss(reduction='none')\n",
    "    \n",
    "    #compare \n",
    "    loss_t = loss_fn(teacher_output, target)\n",
    "    loss_s = loss_fn(student_output, target)\n",
    "    \n",
    "    A_p = loss_t >= loss_s\n",
    "    A_n = ~A_p\n",
    "    \n",
    "    loss1 = loss_fn(student_output, target)\n",
    "    loss2 = loss_fn(student_output, teacher_output)\n",
    "    \n",
    "    # print(loss1.shape, loss2.shape, A_p)\n",
    "    \n",
    "    loss = A_p * loss1 + A_n * (loss1 + alpha * loss2)\n",
    "    \n",
    "    return torch.sum(loss)\n",
    "\n",
    "def med_loss1(student_output, teacher_output, med_student, med_teacher, target, alpha=0.05):\n",
    "    med_loss_fn = torch.nn.L1Loss(reduction='none')\n",
    "    \n",
    "    #compare \n",
    "    loss_t = loss_fn(teacher_output, target)\n",
    "    loss_s = loss_fn(student_output, target)\n",
    "    \n",
    "    A_p = loss_t < loss_s\n",
    "    \n",
    "    loss = A_p * alpha * med_loss_fn(med_student, med_teacher)\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "def KD_train(trainloader, testloader, net_snn, net_dnn, net_teacher, rc, teacher_path, nb_inputs, lr=2e-3, nb_epochs=10):\n",
    "    net_teacher.load_state_dict(torch.load(teacher_path))\n",
    "    net_teacher.eval()\n",
    "    test_teacher(testloader, net_teacher, rc, nb_inputs, num_frame=19)\n",
    "    \n",
    "    params = list(net_snn.parameters()) + list(net_dnn.parameters())\n",
    "    optimizer_student = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999))\n",
    "    scheduler_student = torch.optim.lr_scheduler.StepLR(optimizer_student, step_size=200, gamma=0.1)\n",
    "    \n",
    "    #loss_fn = slayer.loss.SpikeTime(time_constant=2, filter_order=2, reduction='mean').to(device)\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    loss_hist = []\n",
    "    \n",
    "    min_ber = 10000000000000000.0\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        loss_hist = []\n",
    "        for x1_local, x2_local, x3_local, y_local in trainloader:\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            x3_local = x3_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            optimizer_student.zero_grad()\n",
    "            \n",
    "            # snn model \n",
    "            output = net_snn(x1_local)\n",
    "            output = net_dnn(output, x2_local, x3_local)\n",
    "            \n",
    "            # lstm model\n",
    "            x3_local = x3_local.view(x3_local.shape[0], nb_inputs, 4)\n",
    "            prev_out = torch.zeros(x3_local.size(0), net_teacher.n_hidden).to(device)\n",
    "            teacher_output, _ = net_teacher(x3_local, prev_out)\n",
    "            \n",
    "            student_loss = mutual_loss(teacher_output.clone().detach(), output, alpha=0.0, target=y_local, mode=True)\n",
    "            #student_loss = loss_fn(output, y_local)\n",
    "            student_loss.backward()\n",
    "\n",
    "            optimizer_student.step()\n",
    "        \n",
    "        test_ber = test(test_loader, net_snn, net_dnn, rc, num_frame=19)\n",
    "        min_ber = min(min_ber, test_ber)\n",
    "        #test_teacher(test_loader, net_teacher, rc, nb_inputs, num_frame=19)  \n",
    "    return min_ber\n",
    "\n",
    "def mutual_train(trainloader, testloader, net_snn, net_dnn, net_teacher, rc, nb_inputs, lr=2e-3, nb_epochs=10):\n",
    "    params = list(net_snn.parameters()) + list(net_dnn.parameters())\n",
    "    optimizer_student = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999))\n",
    "    optimizer_teacher = torch.optim.Adam(net_teacher.parameters(), lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    scheduler_student = torch.optim.lr_scheduler.StepLR(optimizer_student, step_size=100, gamma=0.1)\n",
    "    scheduler_teacher = torch.optim.lr_scheduler.StepLR(optimizer_teacher, step_size=100, gamma=0.1)\n",
    "    \n",
    "    #loss_fn = slayer.loss.SpikeTime(time_constant=2, filter_order=2, reduction='mean').to(device)\n",
    "    #loss_fn = torch.nn.SmoothL1Loss()\n",
    "    loss_hist = []\n",
    "    min_ber = 10000000000000000.0\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        loss_hist = []\n",
    "        for x1_local, x2_local, x3_local, y_local in trainloader:\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            x3_local = x3_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            optimizer_student.zero_grad()\n",
    "            optimizer_teacher.zero_grad()\n",
    "            \n",
    "            # snn model \n",
    "            output = net_snn(x1_local)\n",
    "            output = net_dnn(output, x2_local, x3_local)\n",
    "            \n",
    "            # lstm model\n",
    "            x3_local = x3_local.view(x3_local.shape[0], nb_inputs, 4)\n",
    "            prev_out = torch.zeros(x3_local.size(0), net_teacher.n_hidden).to(device)\n",
    "            teacher_output, _ = net_teacher(x3_local, prev_out)\n",
    "            \n",
    "            student_loss = mutual_loss(teacher_output.clone().detach(), output, alpha=0.05, target=y_local, mode=True)\n",
    "            teacher_loss = mutual_loss(output.clone().detach(), teacher_output, alpha=0.01, target=y_local, mode=False)\n",
    "            student_loss.backward()\n",
    "            teacher_loss.backward()\n",
    "            optimizer_student.step()\n",
    "            optimizer_teacher.step()\n",
    "            loss_hist.append(student_loss.item() + teacher_loss.item())\n",
    "        \n",
    "        test_ber = test(test_loader, net_snn, net_dnn, rc, num_frame=19)\n",
    "        test_teacher(test_loader, net_teacher, rc, nb_inputs, num_frame=19)\n",
    "        min_ber = min(min_ber, test_ber)\n",
    "    return min_ber\n",
    "\n",
    "\n",
    "def mutual_train_med(trainloader, testloader, net_snn, net_dnn, net_teacher, rc, nb_inputs, lr=2e-3, nb_epochs=10):\n",
    "    params = list(net_snn.parameters()) + list(net_dnn.parameters())\n",
    "    optimizer_student = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    optimizer_teacher = torch.optim.Adam(net_teacher.parameters(), lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    scheduler_student = torch.optim.lr_scheduler.StepLR(optimizer_student, step_size=30, gamma=0.1)\n",
    "    scheduler_teacher = torch.optim.lr_scheduler.StepLR(optimizer_teacher, step_size=30, gamma=0.1)\n",
    "    \n",
    "    #loss_fn = slayer.loss.SpikeTime(time_constant=2, filter_order=2, reduction='mean').to(device)\n",
    "    #loss_fn = torch.nn.SmoothL1Loss()\n",
    "    loss_hist = []\n",
    "    min_ber = 10000000000000000.0\n",
    "    min_teacher_ber = 10000000000000000.0\n",
    "    \n",
    "    spike_med_size = 1\n",
    "    rc_med_size = 32\n",
    "    \n",
    "    med_loss_fn = torch.nn.L1Loss()\n",
    "    med_mapping = torch.rand(rc_med_size, spike_med_size, requires_grad = False).to(device)\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        loss_hist = []\n",
    "        for x1_local, x2_local, x3_local, y_local in trainloader:\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            x3_local = x3_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            optimizer_student.zero_grad()\n",
    "            optimizer_teacher.zero_grad()\n",
    "            \n",
    "            # snn model \n",
    "            output_spike_med = net_snn(x1_local)\n",
    "            output = net_dnn(output_spike_med, x2_local, x3_local)\n",
    "            output_spike_med_mean = torch.mean(output_spike_med, dim=-1)\n",
    "            \n",
    "            # lstm model\n",
    "            x3_local = x3_local.view(x3_local.shape[0], nb_inputs, 4)\n",
    "            prev_out = torch.zeros(x3_local.size(0), net_teacher.n_hidden).to(device)\n",
    "            teacher_output, med = net_teacher(x3_local, prev_out)\n",
    "            \n",
    "            student_loss = mutual_loss1(teacher_output.clone().detach(), output, alpha=0.10, target=y_local, mode=True)\n",
    "            teacher_loss = mutual_loss1(output.clone().detach(), teacher_output, alpha=0.05, target=y_local, mode=True)\n",
    "           \n",
    "            # output_spike_med_flatten = torch.flatten(output_spike_med, start_dim=1)\n",
    "            # map_spike_med = torch.matmul(output_spike_med_mean, med_mapping)\n",
    "            map_spike_med = torch.matmul(med, med_mapping)\n",
    "            med_loss = med_loss_fn(output_spike_med_mean, map_spike_med.clone().detach())\n",
    "            \n",
    "            student_loss += 0.1 * med_loss\n",
    "            \n",
    "            student_loss.backward()\n",
    "            teacher_loss.backward()\n",
    "            \n",
    "            optimizer_student.step()\n",
    "            optimizer_teacher.step()\n",
    "            loss_hist.append(student_loss.item() + teacher_loss.item())\n",
    "        \n",
    "        test_ber = test(test_loader, net_snn, net_dnn, rc, num_frame=19)\n",
    "        test_teacher_ber = test_teacher(test_loader, net_teacher, rc, nb_inputs, num_frame=19)\n",
    "        min_ber = min(min_ber, test_ber)\n",
    "        min_teacher_ber = min(min_teacher_ber, test_teacher_ber)\n",
    "    return min_ber, min_teacher_ber\n",
    "\n",
    "\n",
    "def mutual_train_1(trainloader, testloader, net_snn, net_dnn, net_teacher, rc, nb_inputs, lr=2e-3, nb_epochs=10):\n",
    "    params = list(net_snn.parameters()) + list(net_dnn.parameters())\n",
    "    #params = net_dnn.parameters()\n",
    "    optimizer_student = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    optimizer_teacher = torch.optim.Adam(net_teacher.parameters(), lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    scheduler_student = torch.optim.lr_scheduler.StepLR(optimizer_student, step_size=30, gamma=0.1)\n",
    "    scheduler_teacher = torch.optim.lr_scheduler.StepLR(optimizer_teacher, step_size=30, gamma=0.1)\n",
    "    \n",
    "    #loss_fn = slayer.loss.SpikeTime(time_constant=2, filter_order=2, reduction='mean').to(device)\n",
    "    #loss_fn = torch.nn.SmoothL1Loss()\n",
    "    loss_hist = []\n",
    "    min_ber = 10000000000000000.0\n",
    "    min_teacher_ber = 10000000000000000.0\n",
    "    \n",
    "    spike_med_size = 100\n",
    "    rc_med_size = 32\n",
    "    \n",
    "    med_loss_fn = torch.nn.L1Loss()\n",
    "    med_mapping = torch.rand(spike_med_size, rc_med_size, requires_grad = False).to(device)\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        loss_hist = []\n",
    "        for x1_local, x2_local, x3_local, y_local in trainloader:\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            x3_local = x3_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            optimizer_student.zero_grad()\n",
    "            optimizer_teacher.zero_grad()\n",
    "            \n",
    "            # snn model \n",
    "            output_spike_med = net_snn(x1_local)\n",
    "            output = net_dnn(output_spike_med, x2_local, x3_local)\n",
    "            student_loss = med_loss_fn(output, y_local)\n",
    "            \n",
    "            student_loss.backward()\n",
    "            \n",
    "            optimizer_student.step()\n",
    "        \n",
    "        test_ber = test(test_loader, net_snn, net_dnn, rc, num_frame=19)\n",
    "        #test_teacher_ber = test_teacher(test_loader, net_teacher, rc, nb_inputs, num_frame=19)\n",
    "        min_ber = min(min_ber, test_ber)\n",
    "        #min_teacher_ber = min(min_teacher_ber, test_teacher_ber)\n",
    "    return min_ber\n",
    "\n",
    "def mutual_train_2(trainloader, testloader, net_snn, net_dnn, net_teacher, rc, nb_inputs, lr=2e-3, nb_epochs=10):\n",
    "    params = list(net_snn.parameters()) + list(net_dnn.parameters())\n",
    "    optimizer_student = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    optimizer_teacher = torch.optim.Adam(net_teacher.parameters(), lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    scheduler_student = torch.optim.lr_scheduler.StepLR(optimizer_student, step_size=30, gamma=0.1)\n",
    "    scheduler_teacher = torch.optim.lr_scheduler.StepLR(optimizer_teacher, step_size=30, gamma=0.1)\n",
    "    \n",
    "    #loss_fn = slayer.loss.SpikeTime(time_constant=2, filter_order=2, reduction='mean').to(device)\n",
    "    #loss_fn = torch.nn.SmoothL1Loss()\n",
    "    loss_hist = []\n",
    "    min_ber = 10000000000000000.0\n",
    "    min_teacher_ber = 10000000000000000.0\n",
    "    \n",
    "    spike_med_size = 100\n",
    "    rc_med_size = 32\n",
    "    \n",
    "    med_loss_fn = torch.nn.L1Loss()\n",
    "    med_mapping = torch.rand(spike_med_size, rc_med_size, requires_grad = False).to(device)\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        loss_hist = []\n",
    "        for x1_local, x2_local, x3_local, y_local in trainloader:\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            x3_local = x3_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            optimizer_student.zero_grad()\n",
    "            optimizer_teacher.zero_grad()\n",
    "            \n",
    "            # snn model \n",
    "            output_spike_med = net_snn(x1_local)\n",
    "            output = net_dnn(output_spike_med, x2_local, x3_local)\n",
    "            \n",
    "            # lstm model\n",
    "            x3_local = x3_local.view(x3_local.shape[0], nb_inputs, 4)\n",
    "            prev_out = torch.zeros(x3_local.size(0), net_teacher.n_hidden).to(device)\n",
    "            teacher_output, med = net_teacher(x3_local, prev_out)\n",
    "            \n",
    "            student_loss = mutual_loss1(teacher_output.clone().detach(), output, alpha=0.10, target=y_local, mode=True)\n",
    "            teacher_loss = mutual_loss1(output.clone().detach(), teacher_output, alpha=0.05, target=y_local, mode=True)\n",
    "            \n",
    "            student_loss.backward()\n",
    "            teacher_loss.backward()\n",
    "            optimizer_student.step()\n",
    "            optimizer_teacher.step()\n",
    "        \n",
    "        test_ber = test(test_loader, net_snn, net_dnn, rc, num_frame=19)\n",
    "        #test_teacher_ber = test_teacher(test_loader, net_teacher, rc, nb_inputs, num_frame=19)\n",
    "        min_ber = min(min_ber, test_ber)\n",
    "        #min_teacher_ber = min(min_teacher_ber, test_teacher_ber)\n",
    "    return min_ber\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663fb05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "0.03388886815819279 -0.033966198217177786 -1.3872054205025038e-05 0.004555123840129171\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m net_dnn \u001b[38;5;241m=\u001b[39m DNNNetwork(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m nb_steps \u001b[38;5;241m+\u001b[39m nb_inputs \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m*\u001b[39m nb_inputs, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    126\u001b[0m net_rc \u001b[38;5;241m=\u001b[39m TOriFloatDFRSystem(n_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, n_fc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 127\u001b[0m min_mu_ber, min_teacher_ber \u001b[38;5;241m=\u001b[39m \u001b[43mmutual_train_med\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_snn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_dnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_rc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m overall_mu_ber \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m min_mu_ber\n\u001b[1;32m    129\u001b[0m overall_teacher_ber \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m min_teacher_ber\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mmutual_train_med\u001b[0;34m(trainloader, testloader, net_snn, net_dnn, net_teacher, rc, nb_inputs, lr, nb_epochs)\u001b[0m\n\u001b[1;32m    172\u001b[0m optimizer_teacher\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# snn model \u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m output_spike_med \u001b[38;5;241m=\u001b[39m \u001b[43mnet_snn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1_local\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m output \u001b[38;5;241m=\u001b[39m net_dnn(output_spike_med, x2_local, x3_local)\n\u001b[1;32m    177\u001b[0m output_spike_med_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(output_spike_med, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lava_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, spike)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, spike):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 39\u001b[0m         spike \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spike\n",
      "File \u001b[0;32m~/anaconda3/envs/lava_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/lava_test/lib/python3.10/site-packages/lava/lib/dl/slayer/block/base.py:312\u001b[0m, in \u001b[0;36mAbstractAffine.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# x = self.neuron.dynamics(z)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# voltage or imag state\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lava_test/lib/python3.10/site-packages/lava/lib/dl/slayer/neuron/cuba.py:372\u001b[0m, in \u001b[0;36mNeuron.dynamics\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     current \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(current)\n\u001b[1;32m    367\u001b[0m voltage \u001b[38;5;241m=\u001b[39m leaky_integrator\u001b[38;5;241m.\u001b[39mdynamics(\n\u001b[1;32m    368\u001b[0m     current,  \u001b[38;5;66;03m# bias can be enabled by adding it here\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     quantize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoltage_decay),\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoltage_state\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_scale,\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold_eps\u001b[49m,\n\u001b[1;32m    373\u001b[0m     debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug\n\u001b[1;32m    374\u001b[0m )\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersistent_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;66;03m# The state at last time step needs to be cloned. Otherwise,\u001b[39;00m\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;66;03m# the previous result will be affected since\u001b[39;00m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;66;03m# it will use same memory space.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "silent = True\n",
    "method = 'RLS'  # RLS; INV; INV+RLS\n",
    "# N_total_frame = 17\n",
    "N_total_frame = 94\n",
    "N_sync_frame = 4\n",
    "# SNR_list = np.arange(1,20,2)\n",
    "SNR_list = [0]\n",
    "\n",
    "# Dataset selection\n",
    "folder_name = 'data/S2/'  # LOS_Near:S2, LOS_Far:S3, NLOS:S1\n",
    "output_folder = 'data_outputs/S1'\n",
    "\n",
    "if folder_name == 'data/S1/':  # NLOS\n",
    "    delay = 0\n",
    "    packet_num = 21\n",
    "elif folder_name == 'data/S2/':  # LOS_Near\n",
    "    delay = 1\n",
    "    packet_num = 27 # correct\n",
    "elif folder_name == 'data/S3/':  # LOS_Far\n",
    "    delay = 1\n",
    "    packet_num = 22 # 23\n",
    "else:\n",
    "    print(\"Undefined Dataset\")\n",
    "    exit(1)\n",
    "    \n",
    "window_size = 2\n",
    "N_reservoir = 16\n",
    "debug = False\n",
    "\n",
    "ber_record = []\n",
    "dfe_ber_record = []\n",
    "LS_ber_record = []\n",
    "comb_ber_record = []\n",
    "sta_ber_record = []\n",
    "tanh_lut = tanh(\n",
    "    input_bit=8,\n",
    "    dx_bit=8,\n",
    "    slope_fmt=(10, 10),\n",
    "    intercept_fmt=(19, 19),\n",
    "    max=8,\n",
    "    better_lut=True,\n",
    "    verbose=False,\n",
    "    plot=False)\n",
    "\n",
    "SNR = SNR_list[0]\n",
    "\n",
    "overall_mu_ber = 0.0\n",
    "overall_teacher_ber = 0.0\n",
    "overall_base_ber = 0.0\n",
    "overall_nopath_ber = 0.0\n",
    "cnt = 0\n",
    "\n",
    "# except 2\n",
    "packets =[13, 14, 16, 17, 18, 19, 20]  # [11, 13, 14, 16,]\n",
    "for i in packets:\n",
    "    print(i)\n",
    "    rc = RC(silent, method, N_total_frame, N_sync_frame, SNR, delay, window_size, i,\n",
    "            N_reservoir=16,\n",
    "            spectral_radius=0.2,\n",
    "            sparsity=0.4,\n",
    "            noise=1e-6,\n",
    "            lut_activation=False,  # True,\n",
    "            tanh_lut=tanh_lut,\n",
    "            input_scale=25,  #40, #50, # 25,\n",
    "            reservoir_input_scale = 8,  #4,  #5,\n",
    "            show_wout=False,\n",
    "            output_folder= output_folder,\n",
    "            debug=debug,\n",
    "            use_fpga= None,\n",
    "            sock= None,  # usock\n",
    "            addr = None) # addr\n",
    "\n",
    "    train_input, train_label, test_input, test_label = rc.run()\n",
    "    train_max = np.max(train_input)\n",
    "    train_min = np.min(train_input)\n",
    "    train_mean = np.mean(train_input)\n",
    "    train_std = np.std(train_input)\n",
    "    print(train_max, train_min, train_mean, train_std)\n",
    "   \n",
    "    #RC_test_input = np.load('gt_test_input_1.npy')\n",
    "    #RC_train_input = np.load('gt_train_input_1.npy')\n",
    "    #RC_train_label = np.load('gt_train_label_1.npy')\n",
    " \n",
    "    #print(RC_test_input.shape, test_input.shape)\n",
    "    #print(\"test_input_diff: \", torch.nn.MSELoss()(torch.tensor(RC_test_input), torch.tensor(test_input)))\n",
    "\n",
    "    #print(\"train_input_diff: \", torch.nn.MSELoss()(torch.tensor(RC_train_input), torch.tensor(train_input)))\n",
    "\n",
    "    #print(\"train_label_diff: \", torch.nn.MSELoss()(torch.tensor(RC_train_label), torch.tensor(train_label)))\n",
    "\n",
    "    train_mean = np.mean(train_input)\n",
    "    train_std = np.std(train_input)\n",
    "\n",
    "    train_input = (train_input - train_mean) / train_std\n",
    "    test_input = (test_input - train_mean) / train_std\n",
    "    train_label = 1.0 * train_label\n",
    "    \n",
    "    train_input, train_label, test_input, test_label = pre_processing(train_input, train_label)\n",
    "    \n",
    "    \n",
    "    nb_inputs  = 6\n",
    "    nb_steps  = 100\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    import scipy.io\n",
    "\n",
    "    from dataset import Dataset, RegTorchSeasonalitySpikingDataset, RegSpikingDataset, RegTorchSpikingDataset, RegTorchSeasonalityLinearSpikingDataset\n",
    "    train_data = RegTorchSeasonalityLinearSpikingDataset(train_input, train_label, nb_inputs, nb_steps)\n",
    "    test_data = RegTorchSeasonalityLinearSpikingDataset(test_input, test_label, nb_inputs, nb_steps)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "\n",
    "    # KD learning\n",
    "    #net_snn = Network(nb_inputs * 4, 2).to(device)\n",
    "    #net_dnn = DNNNetwork(1 * nb_steps + nb_inputs * 4 + 0 + 0 * nb_inputs, 2).to(device)\n",
    "    #net_rc = TOriFloatDFRSystem(n_hidden=16, n_fc=64)\n",
    "    \n",
    "    #teacher_min_ber = train_dnn(train_loader, test_loader, net_rc, rc, nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    #KD_min_ber = KD_train(train_loader, test_loader, net_snn, net_dnn, net_rc, rc, './teacher.pt', nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    \n",
    "    # mutual learning\n",
    "    net_snn = Network(nb_inputs * 4, 2).to(device)\n",
    "    net_dnn = DNNNetwork(1 * nb_steps + nb_inputs * 4 + 0 + 0 * nb_inputs, 2).to(device)\n",
    "    net_rc = TOriFloatDFRSystem(n_hidden=16, n_fc=32).to(device)\n",
    "    min_mu_ber, min_teacher_ber = mutual_train_med(train_loader, test_loader, net_snn, net_dnn, net_rc, rc, nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    overall_mu_ber += min_mu_ber\n",
    "    overall_teacher_ber += min_teacher_ber\n",
    "    print(min_mu_ber, min_teacher_ber)\n",
    "    \"\"\"\n",
    "    #base model\n",
    "    net_snn = Network(nb_inputs * 4, 2).to(device)\n",
    "    net_dnn = DNNNetwork(1 * nb_steps + nb_inputs * 4 + 0 + 0 * nb_inputs, 2).to(device)\n",
    "    min_base_ber = mutual_train_1(train_loader, test_loader, net_snn, net_dnn, net_rc, rc, nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    overall_base_ber += min_base_ber\n",
    "    \n",
    "    # no path\n",
    "    net_snn = Network(nb_inputs * 4, 2).to(device)\n",
    "    net_dnn = DNNNetwork(1 * nb_steps + nb_inputs * 4 + 0 + 0 * nb_inputs, 2).to(device)\n",
    "    net_rc = TOriFloatDFRSystem(n_hidden=16, n_fc=32).to(device)\n",
    "    min_nopath_ber = mutual_train_2(train_loader, test_loader, net_snn, net_dnn, net_rc, rc, nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    overall_nopath_ber += min_nopath_ber\n",
    "    \n",
    "    cnt += 1\n",
    "    \n",
    "    print(min_base_ber, min_mu_ber, min_teacher_ber, min_nopath_ber)\n",
    "    \"\"\"\n",
    "\n",
    "print(\"ML SNN: \", overall_mu_ber / len(packets))\n",
    "print(\"teacher: \", overall_teacher_ber / len(packets))\n",
    "print(\"base: \", overall_base_ber / len(packets))\n",
    "print(\"nopath: \", overall_nopath_ber / len(packets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8f188-a209-47ae-92ff-154ba57d6378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lava_test] *",
   "language": "python",
   "name": "conda-env-lava_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
