{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138b6027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cpu\n",
      "init done\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "#import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from dataset import Dataset, SpikingDataset, RegSpikingDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from Loss import KDLoss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "random.seed(1338)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pyESN import ESN\n",
    "from scipy import interpolate\n",
    "from gen_data import *\n",
    "from tanh import tanh\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "torch.__version__\n",
    "# The coarse network structure is dicated by the Fashion MNIST dataset. \n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')\n",
    "    #device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "time_step = 1e-3\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
    "\n",
    "print(\"init done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663fb05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7521, 4) (7521, 4)\n",
      "test_input_diff:  tensor(4.3559e-10, dtype=torch.float64)\n",
      "train_input_diff:  tensor(4.6755e-11, dtype=torch.float64)\n",
      "train_label_diff:  tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "silent = True\n",
    "method = 'RLS'  # RLS; INV; INV+RLS\n",
    "# N_total_frame = 17\n",
    "N_total_frame = 94\n",
    "N_sync_frame = 4\n",
    "# SNR_list = np.arange(1,20,2)\n",
    "SNR_list = [55]\n",
    "\n",
    "# Dataset selection\n",
    "folder_name = 'data/S2/'  # LOS_Near:S2, LOS_Far:S3, NLOS:S1\n",
    "output_folder = 'data_outputs/S1'\n",
    "\n",
    "if folder_name == 'data/S1/':  # NLOS\n",
    "    delay = 0\n",
    "    packet_num = 21\n",
    "elif folder_name == 'data/S2/':  # LOS_Near\n",
    "    delay = 1\n",
    "    packet_num = 27 # correct\n",
    "elif folder_name == 'data/S3/':  # LOS_Far\n",
    "    delay = 1\n",
    "    packet_num = 22 # 23\n",
    "else:\n",
    "    print(\"Undefined Dataset\")\n",
    "    exit(1)\n",
    "    \n",
    "window_size = 2\n",
    "N_reservoir = 16\n",
    "debug = False\n",
    "\n",
    "ber_record = []\n",
    "dfe_ber_record = []\n",
    "LS_ber_record = []\n",
    "comb_ber_record = []\n",
    "sta_ber_record = []\n",
    "tanh_lut = tanh(\n",
    "    input_bit=8,\n",
    "    dx_bit=8,\n",
    "    slope_fmt=(10, 10),\n",
    "    intercept_fmt=(19, 19),\n",
    "    max=8,\n",
    "    better_lut=True,\n",
    "    verbose=False,\n",
    "    plot=False)\n",
    "\n",
    "SNR = SNR_list[0]\n",
    "i = 1\n",
    "rc = RC(silent, method, N_total_frame, N_sync_frame, SNR, delay, window_size, i,\n",
    "        N_reservoir=16,\n",
    "        spectral_radius=0.2,\n",
    "        sparsity=0.4,\n",
    "        noise=1e-6,\n",
    "        lut_activation=False,  # True,\n",
    "        tanh_lut=tanh_lut,\n",
    "        input_scale=25,  #40, #50, # 25,\n",
    "        reservoir_input_scale = 8,  #4,  #5,\n",
    "        show_wout=False,\n",
    "        output_folder= output_folder,\n",
    "        debug=debug,\n",
    "        use_fpga= None,\n",
    "        sock= None,  # usock\n",
    "        addr = None) # addr\n",
    "\n",
    "train_input, train_label, test_input, test_label = rc.run()\n",
    "RC_test_input = np.load('gt_test_input_1.npy')\n",
    "RC_train_input = np.load('gt_train_input_1.npy')\n",
    "RC_train_label = np.load('gt_train_label_1.npy')\n",
    "\n",
    "print(RC_test_input.shape, test_input.shape)\n",
    "print(\"test_input_diff: \", torch.nn.MSELoss()(torch.tensor(RC_test_input), torch.tensor(test_input)))\n",
    "\n",
    "print(\"train_input_diff: \", torch.nn.MSELoss()(torch.tensor(RC_train_input), torch.tensor(train_input)))\n",
    "\n",
    "print(\"train_label_diff: \", torch.nn.MSELoss()(torch.tensor(RC_train_label), torch.tensor(train_label)))\n",
    "\n",
    "train_mean = np.mean(train_input)\n",
    "train_std = np.std(train_input)\n",
    "\n",
    "train_input = (train_input - train_mean) / train_std\n",
    "test_input = (test_input - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b561f180-c49b-4dea-b569-24ce1805dc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.70710677+0.70710677j  0.70710677-0.70710677j -0.70710677+0.70710677j\n",
      " -0.70710677-0.70710677j -0.70710677+0.70710677j -1.        +0.j\n",
      "  0.70710677-0.70710677j  0.70710677-0.70710677j  0.70710677+0.70710677j\n",
      " -0.70710677-0.70710677j -0.70710677+0.70710677j  0.70710677+0.70710677j\n",
      "  0.        +0.j          0.70710677+0.70710677j -0.70710677+0.70710677j\n",
      " -0.70710677-0.70710677j -0.70710677+0.70710677j  0.70710677-0.70710677j\n",
      "  0.70710677+0.70710677j -1.        +0.j          0.70710677+0.70710677j\n",
      "  0.70710677-0.70710677j  0.70710677-0.70710677j  0.70710677-0.70710677j\n",
      "  0.70710677+0.70710677j -0.70710677-0.70710677j -0.70710677-0.70710677j\n",
      " -0.70710677+0.70710677j  0.70710677+0.70710677j -0.70710677+0.70710677j\n",
      "  0.70710677-0.70710677j  0.70710677+0.70710677j -0.70710677+0.70710677j\n",
      "  1.        +0.j          0.70710677+0.70710677j -0.70710677+0.70710677j\n",
      " -0.70710677+0.70710677j -0.70710677+0.70710677j -0.70710677+0.70710677j\n",
      "  0.        +0.j          0.        +0.j          0.        +0.j\n",
      "  0.        +0.j          0.        +0.j        ]\n"
     ]
    }
   ],
   "source": [
    "print(test_label[-26, 20 : 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a8c5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7521, 4)\n",
      "(7521, 4)\n",
      "(7521, 2)\n",
      "(89, 64)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(test_input.shape)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7395dfe-dc16-458e-989e-b39bfcc397e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           1         2         3         4        L1        L2  L_0  L_1  L_2  \\\n",
      "0   0.004553  0.004553 -1.888441  0.270612 -0.002946  0.002839    1    0    0   \n",
      "1  -1.888441  0.270612  0.812097 -5.109579 -0.007376  0.005062    0    1    0   \n",
      "2   0.812097 -5.109579  2.425456  1.353290 -0.005515 -0.000335    0    0    1   \n",
      "3   2.425456  1.353290  0.278971  5.126545  0.005362 -0.004129    0    0    0   \n",
      "4   0.278971  5.126545  0.012743  3.788194  0.005562 -0.000661    0    0    0   \n",
      "5   0.012743  3.788194  1.354605  4.865612  0.003752  0.000764    0    0    0   \n",
      "6   1.354605  4.865612  2.707468 -2.700189  0.004581  0.003351    0    0    0   \n",
      "7   2.707468 -2.700189 -1.343801 -4.040178  0.003330  0.003798    0    0    0   \n",
      "8  -1.343801 -4.040178 -0.263666  2.431761 -0.003775 -0.003522    0    0    0   \n",
      "9  -0.263666  2.431761  5.128487 -1.344931 -0.009681 -0.004466    0    0    0   \n",
      "10  5.128487 -1.344931 -1.625135 -2.413300 -0.002946  0.002839    1    0    0   \n",
      "11 -1.625135 -2.413300 -5.120997  0.010228 -0.007376  0.005062    0    1    0   \n",
      "12 -5.120997  0.010228 -4.044805  0.281725 -0.005515 -0.000335    0    0    1   \n",
      "13 -4.044805  0.281725 -4.854996 -0.788700  0.005362 -0.004129    0    0    0   \n",
      "14 -4.854996 -0.788700  1.876893 -2.966286  0.005562 -0.000661    0    0    0   \n",
      "15  1.876893 -2.966286  3.786988  0.810478  0.003752  0.000764    0    0    0   \n",
      "16  3.786988  0.810478 -1.893726  0.538356  0.004581  0.003351    0    0    0   \n",
      "17 -1.893726  0.538356  0.531661 -5.125171  0.003330  0.003798    0    0    0   \n",
      "18  0.531661 -5.125171  2.698997  0.817785 -0.003775 -0.003522    0    0    0   \n",
      "19  2.698997  0.817785  0.565624  5.139858 -0.009681 -0.004466    0    0    0   \n",
      "\n",
      "    L_3  L_4  L_5  L_6  L_7  L_8  L_9  \n",
      "0     0    0    0    0    0    0    0  \n",
      "1     0    0    0    0    0    0    0  \n",
      "2     0    0    0    0    0    0    0  \n",
      "3     1    0    0    0    0    0    0  \n",
      "4     0    1    0    0    0    0    0  \n",
      "5     0    0    1    0    0    0    0  \n",
      "6     0    0    0    1    0    0    0  \n",
      "7     0    0    0    0    1    0    0  \n",
      "8     0    0    0    0    0    1    0  \n",
      "9     0    0    0    0    0    0    1  \n",
      "10    0    0    0    0    0    0    0  \n",
      "11    0    0    0    0    0    0    0  \n",
      "12    0    0    0    0    0    0    0  \n",
      "13    1    0    0    0    0    0    0  \n",
      "14    0    1    0    0    0    0    0  \n",
      "15    0    0    1    0    0    0    0  \n",
      "16    0    0    0    1    0    0    0  \n",
      "17    0    0    0    0    1    0    0  \n",
      "18    0    0    0    0    0    1    0  \n",
      "19    0    0    0    0    0    0    1  \n",
      "(7521, 16)\n",
      "          1         2         3         4        L1        L2  L_0  L_1  L_2  \\\n",
      "0  0.004553  0.004553 -1.888441  0.270612 -0.002946  0.002839    1    0    0   \n",
      "1 -1.888441  0.270612  0.812097 -5.109579 -0.007376  0.005062    0    1    0   \n",
      "2  0.812097 -5.109579  2.425456  1.353290 -0.005515 -0.000335    0    0    1   \n",
      "3  2.425456  1.353290  0.278971  5.126545  0.005362 -0.004129    0    0    0   \n",
      "4  0.278971  5.126545  0.012743  3.788194  0.005562 -0.000661    0    0    0   \n",
      "\n",
      "   L_3  L_4  L_5  L_6  L_7  L_8  L_9  \n",
      "0    0    0    0    0    0    0    0  \n",
      "1    0    0    0    0    0    0    0  \n",
      "2    0    0    0    0    0    0    0  \n",
      "3    1    0    0    0    0    0    0  \n",
      "4    0    1    0    0    0    0    0  \n",
      "(7521, 16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def pre_processing(train_input, test_input, train_label):\n",
    "    idx_p = 10\n",
    "    begin = -1 # N_total_frame * N_sync_frame\n",
    "\n",
    "    train_label_df = pd.DataFrame(train_label, columns = ['L1','L2'])\n",
    "\n",
    "    train_df = pd.DataFrame(train_input, columns = ['1','2', '3', '4'])\n",
    "    train_df['L1_idx'] = train_df.index % idx_p\n",
    " \n",
    "    train_label_df['L1_idx'] = train_label_df.index % idx_p\n",
    "\n",
    "    # group by \n",
    "    mapping = train_label_df.loc[begin:, :].groupby(by='L1_idx').mean().reset_index().loc[:, ['L1', 'L2', 'L1_idx']] \n",
    "    \n",
    "    train_df = pd.merge(train_df, mapping, how='left', on='L1_idx')\n",
    "\n",
    "    train_df = pd.get_dummies(train_df, prefix=['L'], columns=['L1_idx'])\n",
    "\n",
    "    train_df = train_df.loc[begin:, :]\n",
    "\n",
    "\n",
    "    print(train_df.head(20))\n",
    "    train_input = train_df.to_numpy()\n",
    "    print(train_input.shape)\n",
    "\n",
    "    test_df = pd.DataFrame(test_input, columns = ['1','2', '3', '4'])\n",
    "    test_df['L1_idx'] = test_df.index % idx_p\n",
    "\n",
    "    #group by\n",
    "    test_df = test_df.merge(mapping, how = 'left', on='L1_idx')\n",
    "\n",
    "    test_df = pd.get_dummies(test_df, prefix=['L'], columns=['L1_idx'])\n",
    "\n",
    "    print(test_df.head())\n",
    "    test_input = test_df.to_numpy()\n",
    "    print(test_input.shape)\n",
    "    \n",
    "    return train_input, test_input\n",
    "\n",
    "train_input, test_input = pre_processing(train_input, test_input, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0acdc9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7521\n",
      "(32, 100)\n",
      "(1,)\n",
      "(0,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "nb_inputs  = 8\n",
    "nb_hidden  = 96\n",
    "nb_outputs = 2\n",
    "\n",
    "time_step = 1e-3\n",
    "nb_steps  = 100\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "from dataset import Dataset, RegTorchSeasonalitySpikingDataset, RegSpikingDataset, RegTorchSpikingDataset\n",
    "train_data = RegTorchSeasonalitySpikingDataset(train_input, train_label, nb_inputs, nb_steps)\n",
    "test_data = RegTorchSeasonalitySpikingDataset(test_input, train_label, nb_inputs, nb_steps)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "print(len(train_data))\n",
    "print(train_data[0][0].shape)\n",
    "print(train_data[0][1].shape)\n",
    "print(train_data[0][2].shape)\n",
    "print(train_data[0][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf4c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "       # neuron_params = {\n",
    "       #         'threshold'     : 0.1,\n",
    "       #         'current_decay' : 1,\n",
    "       #         'voltage_decay' : 0.1,\n",
    "       #         'requires_grad' : True,     \n",
    "       #     }\n",
    "        #neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        neuron_params = {\n",
    "                'threshold'     : 1.25,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 100,\n",
    "                'requires_grad' : True,     \n",
    "            }\n",
    "        \n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                #slayer.block.cuba.Input(neuron_params),\n",
    "                slayer.block.cuba.Recurrent(neuron_params, input_size, 64, weight_norm=True, delay=True),\n",
    "                #slayer.block.cuba.Dense(neuron_params, 32, 64, weight_norm=True, delay=True),\n",
    "                slayer.block.cuba.Dense(neuron_params, 64, 128, weight_norm=True, delay=True),\n",
    "                slayer.block.cuba.Dense(neuron_params, 128, output_size, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, input_size, 64, weight_scale=2, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, 64, 128, weight_scale=2, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, 128, output_size, weight_scale=2, weight_norm=True)\n",
    "                #slayer.block.cuba.Recurrent(cuba_params, 100, 50),\n",
    "                #slayer.block.cuba.KWTA(cuba_params, 50, 50, num_winners=5)\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))\n",
    "\n",
    "class DNNNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16 + 1 + 12, 8)\n",
    "        self.fc3 = nn.Linear(8, 2)\n",
    "        #self.fc3 = nn.Linear(128, 128)\n",
    "        #self.fc4 = nn.Linear(128, 2)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self,x, x1, x2):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        #x = torch.cat((x, x1), axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x) \n",
    "        x = torch.cat((x, x1, x2), axis=1)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc3(x)\n",
    "        #x = self.fc1(x)\n",
    "        #x = self.act(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = self.act(x)\n",
    "        #x = self.fc3(x)\n",
    "        #x = self.act(x)\n",
    "        #x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1084390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.neuron.current_decay torch.Size([1])\n",
      "blocks.0.neuron.voltage_decay torch.Size([1])\n",
      "blocks.0.delay.delay torch.Size([1])\n",
      "blocks.0.input_synapse.weight_g torch.Size([64, 1, 1, 1, 1])\n",
      "blocks.0.input_synapse.weight_v torch.Size([64, 32, 1, 1, 1])\n",
      "blocks.0.recurrent_synapse.weight_g torch.Size([64, 1, 1, 1, 1])\n",
      "blocks.0.recurrent_synapse.weight_v torch.Size([64, 64, 1, 1, 1])\n",
      "blocks.1.neuron.current_decay torch.Size([1])\n",
      "blocks.1.neuron.voltage_decay torch.Size([1])\n",
      "blocks.1.delay.delay torch.Size([1])\n",
      "blocks.1.synapse.weight_g torch.Size([128, 1, 1, 1, 1])\n",
      "blocks.1.synapse.weight_v torch.Size([128, 64, 1, 1, 1])\n",
      "blocks.2.neuron.current_decay torch.Size([1])\n",
      "blocks.2.neuron.voltage_decay torch.Size([1])\n",
      "blocks.2.synapse.weight_g torch.Size([2, 1, 1, 1, 1])\n",
      "blocks.2.synapse.weight_v torch.Size([2, 128, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "net_snn = Network(nb_inputs * 4, 2).to(device)\n",
    "net_dnn = DNNNetwork(2 * 100, 2).to(device)\n",
    "\n",
    "for name, weight in net_snn.named_parameters():\n",
    "    print(name, weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db8441bd-8c48-4451-a52d-88763313aeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_533/3076915975.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).float()\n",
      "/tmp/ipykernel_533/3076915975.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).float()\n",
      "/tmp/ipykernel_533/3076915975.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input1 = torch.tensor(input1).float()\n",
      "/tmp/ipykernel_533/3076915975.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input2 = torch.tensor(input2).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE:  tensor(0.3056, dtype=torch.float64)\n",
      "[ 6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 30 31\n",
      " 33 34 35 36 37 38 40 41 42 43 44 45 46 47 48 49 50 51 52 54 55 56 57 58]\n",
      "0.5304307116104869\n"
     ]
    }
   ],
   "source": [
    "RC_test_time = np.load('gt_time.npy')\n",
    "RC_test_freq = np.load('gt_freq.npy')\n",
    "RC_test_input = np.load('gt_test_input_1.npy')\n",
    "\n",
    "def test(test_loader, net_snn, net_dnn):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target)\n",
    "        target = torch.tensor(target).float()\n",
    "        input = torch.tensor(input).float()\n",
    "        input1 = torch.tensor(input1).float()\n",
    "        input2 = torch.tensor(input2).float()\n",
    "    \n",
    "        output = net_snn(input)\n",
    "        output = net_dnn(output, input1, input2).cpu().detach().numpy()\n",
    "        all_output.append(output)\n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    print(\"test MSE: \", torch.nn.MSELoss()(torch.tensor(all_output), torch.tensor(RC_test_time)))\n",
    "    ber = rc.my_test(all_output)\n",
    "    print(ber)\n",
    "    \n",
    "test(test_loader, net_snn, net_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65600ef7-8c7e-4b1e-aec1-9fdc79bc7513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train MSE:  tensor(0.0033, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "RC_train_time = np.load(\"gt_train_pred.npy\")\n",
    "print(\"train MSE: \", torch.nn.MSELoss()(torch.tensor(RC_train_time), torch.tensor(train_label)))\n",
    "def test_train(test_loader, net_snn, net_dnn):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target)\n",
    "        target = torch.tensor(target).float()\n",
    "        input = torch.tensor(input).float()\n",
    "        input1 = torch.tensor(input1).float()\n",
    "        input2 = torch.tensor(input2).float()\n",
    "    \n",
    "        output = net_snn(input)\n",
    "        output = net_dnn(output, input1, input2).cpu().detach().numpy()\n",
    "        all_output.append(output)\n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    print(\"train MSE: \", torch.nn.MSELoss()(torch.tensor(all_output), torch.tensor(RC_train_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bce4a5c2-38de-466c-ab45-ae78ecfc5307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, testloader, model, DNN_model, lr=2e-3, nb_epochs=10):\n",
    "    #params = [w1,w2]\n",
    "    params = list(model.parameters()) + list(DNN_model.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999))\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "    \n",
    "    #loss_fn = slayer.loss.SpikeTime(time_constant=2, filter_order=2, reduction='mean').to(device)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    #loss_fn = torch.nn.SmoothL1Loss()\n",
    "    loss_hist = []\n",
    "    DNN_model.train()\n",
    "    \n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        print(e)\n",
    "        local_loss = []\n",
    "        for x_local, x1_local, x2_local, y_local in trainloader:\n",
    "            x_local = x_local.float().to(device)\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            #output = model(x_local)\n",
    "            #output = output.flatten(start_dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_local)\n",
    "            output = DNN_model(output, x1_local, x2_local)\n",
    "            loss_val = loss_fn(output, y_local) \n",
    "            loss_val.backward()\n",
    "            #print(\"AAAA: \", DNN_model.fc2.weight)\n",
    "            #print(\"BBBB: \", DNN_model.fc1.weight.grad)\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "            \n",
    "        \n",
    "        #if e % 1 == 30 and e != 0:\n",
    "        #    print(\"Training accuracy: %.3f\"%(compute_ber(trainloader, net, \"train\")))\n",
    "        #    print(\"Test accuracy: %.3f\"%(compute_ber(testloader, net, name)))\n",
    "        scheduler.step()\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        print(mean_loss)\n",
    "        #print(\"Epoch %i: loss=%.5f\"%(e+1,mean_loss))\n",
    "        test(testloader, model, DNN_model)\n",
    "        test_train(trainloader, model, DNN_model)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f574380-3878-4cbd-9a38-19da46e5efdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train(train_loader, test_loader, net_snn, net_dnn, lr=1e-3, nb_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac761138-0e7d-42b0-a0ed-c75ab87036ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train MSE:  tensor(0.0033, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_219/3403701246.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).float()\n",
      "/tmp/ipykernel_219/3403701246.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).float()\n",
      "/tmp/ipykernel_219/3403701246.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input1 = torch.tensor(input1).float()\n",
      "/tmp/ipykernel_219/3403701246.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input2 = torch.tensor(input2).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train MSE RC:  tensor(0.0051, dtype=torch.float64)\n",
      "train MSE GT:  tensor(0.0036, dtype=torch.float64)\n",
      "train MSE GT:  tensor(0.0033, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "RC_train_time = np.load(\"gt_train_pred.npy\")\n",
    "print(\"train MSE: \", torch.nn.MSELoss()(torch.tensor(RC_train_time), torch.tensor(train_label)))\n",
    "def test_train_both(train_loader, net_snn, net_dnn):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in train_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target)\n",
    "        target = torch.tensor(target).float()\n",
    "        input = torch.tensor(input).float()\n",
    "        input1 = torch.tensor(input1).float()\n",
    "        input2 = torch.tensor(input2).float()\n",
    "    \n",
    "        output = net_snn(input)\n",
    "        output = net_dnn(output, input1, input2).cpu().detach().numpy()\n",
    "        all_output.append(output)\n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    print(\"train MSE RC: \", torch.nn.MSELoss()(torch.tensor(all_output), torch.tensor(RC_train_time)))\n",
    "    print(\"train MSE GT: \", torch.nn.MSELoss()(torch.tensor(all_output), torch.tensor(train_label)))\n",
    "    print(\"train MSE GT: \", torch.nn.MSELoss()(torch.tensor(train_label), torch.tensor(RC_train_time)))\n",
    "\n",
    "#train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_train_both(train_loader, net_snn, net_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "808d7c6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object dtype dtype('O') has no native HDF5 equivalent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnet_snn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./net_snnaaaaaaaaa.net\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(net_dnn\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./net_dnn.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mNetwork.export_hdf5\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     43\u001b[0m layer \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mcreate_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_group\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/lava/lib/dl/slayer/block/base.py:545\u001b[0m, in \u001b[0;36mAbstractDense.export_hdf5\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# dense descriptors\u001b[39;00m\n\u001b[1;32m    542\u001b[0m handle\u001b[38;5;241m.\u001b[39mcreate_dataset(\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m1\u001b[39m, ), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS10\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m    544\u001b[0m )\n\u001b[0;32m--> 545\u001b[0m \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m handle\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minFeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynapse\u001b[38;5;241m.\u001b[39min_channels)\n\u001b[1;32m    547\u001b[0m handle\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutFeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynapse\u001b[38;5;241m.\u001b[39mout_channels)\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/h5py/_hl/group.py:161\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    158\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    159\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 161\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/.conda/envs/d2l/lib/python3.8/site-packages/h5py/_hl/dataset.py:88\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[0;32m---> 88\u001b[0m     tid \u001b[38;5;241m=\u001b[39m \u001b[43mh5t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Legacy\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[38;5;129;01mand\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/h5t.pyx:1663\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1687\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1747\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object dtype dtype('O') has no native HDF5 equivalent"
     ]
    }
   ],
   "source": [
    "net_snn.export_hdf5('./net_snnaaaaaaaaa.net')\n",
    "torch.save(net_dnn.state_dict(), './net_dnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "898cb37e-9af7-45f8-a3e2-8ddbd0db0dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98/229205384.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).float()\n",
      "/tmp/ipykernel_98/229205384.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).float()\n",
      "/tmp/ipykernel_98/229205384.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input1 = torch.tensor(input1).float()\n",
      "/tmp/ipykernel_98/229205384.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input2 = torch.tensor(input2).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7521, 2]) torch.Size([7521, 2])\n",
      "all_loss:  tensor(0.0024)\n",
      "no first 330 loss:  tensor(0.0014)\n"
     ]
    }
   ],
   "source": [
    "temp_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "def test_MSE(test_loader, net_snn, net_dnn):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "    \n",
    "        target = torch.tensor(target).float()\n",
    "        input = torch.tensor(input).float()\n",
    "        input1 = torch.tensor(input1).float()\n",
    "        input2 = torch.tensor(input2).float()\n",
    "    \n",
    "        output = net_snn(input)\n",
    "        output = net_dnn(output, input1, input2).cpu().detach().numpy()\n",
    "        all_output.append(output)\n",
    "        labels.append(target.cpu().detach().numpy())\n",
    "    \n",
    "    all_output = torch.tensor(np.concatenate(all_output, axis=0)).detach()\n",
    "    labels = torch.tensor(np.concatenate(labels, axis=0)).detach()\n",
    "    print(all_output.shape, labels.shape)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    print(\"all_loss: \", loss_fn(all_output, labels))\n",
    "    print(\"no first 330 loss: \", loss_fn(all_output[330:, :], labels[330:, :]))\n",
    "\n",
    "test_MSE(temp_loader, net_snn, net_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7acff8c-50b2-4bd2-93f8-b3106dddedbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98/3202473027.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).float()\n",
      "/tmp/ipykernel_98/3202473027.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input).float()\n",
      "/tmp/ipykernel_98/3202473027.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input1 = torch.tensor(input1).float()\n",
      "/tmp/ipykernel_98/3202473027.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input2 = torch.tensor(input2).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18902153558052434\n"
     ]
    }
   ],
   "source": [
    "all_output = []\n",
    "inputs = []\n",
    "labels = []\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "def test(test_loader, net_snn, net_dnn):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target)\n",
    "        target = torch.tensor(target).float()\n",
    "        input = torch.tensor(input).float()\n",
    "        input1 = torch.tensor(input1).float()\n",
    "        input2 = torch.tensor(input2).float()\n",
    "    \n",
    "        output = net_snn(input)\n",
    "        output = net_dnn(output, input1, input2).cpu().detach().numpy()\n",
    "        all_output.append(output)\n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    ber = rc.my_test(all_output)\n",
    "    print(ber)\n",
    "    \n",
    "test(test_loader, net_snn, net_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ee219a-952a-4395-b2a9-db85d18f17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9., 9.,  ..., 9., 9., 9.],\n",
      "        [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "        [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "        ...,\n",
      "        [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "        [9., 9., 9.,  ..., 9., 9., 9.],\n",
      "        [9., 9., 9.,  ..., 9., 9., 9.]])\n"
     ]
    }
   ],
   "source": [
    "A1 = 8 * torch.ones((128, 8))\n",
    "A2 = torch.ones((128, 1))\n",
    "print(A1 + A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eebf40-ab07-4dda-9afb-3caa814cefa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l:Python",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
