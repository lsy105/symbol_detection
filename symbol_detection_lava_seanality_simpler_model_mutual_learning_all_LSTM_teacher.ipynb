{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138b6027",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cpu\n",
      "init done\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "#import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from dataset import Dataset, SpikingDataset, RegSpikingDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from Loss import KDLoss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "random.seed(1338)\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pyESN import ESN\n",
    "from scipy import interpolate\n",
    "from gen_data import *\n",
    "from tanh import tanh\n",
    "from DFRSystem import *\n",
    "import pandas as pd\n",
    "\n",
    "from Loss import ber_loss\n",
    "\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import slayer from lava-dl\n",
    "import lava.lib.dl.slayer as slayer\n",
    "\n",
    "import IPython.display as display\n",
    "from matplotlib import animation\n",
    "\n",
    "generating_data = False\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "torch.__version__\n",
    "# The coarse network structure is dicated by the Fashion MNIST dataset. \n",
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    #device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "time_step = 1e-3\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
    "\n",
    "print(\"init done\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe3043c-7ed4-4a5d-b124-5e589297efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(train_input, train_label):\n",
    "    idx_p = 10\n",
    "    begin = 0 # N_total_frame * N_sync_frame\n",
    "    \n",
    "    # label index for data\n",
    "    train_input_df = pd.DataFrame(train_input, columns = ['1','2', '3', '4'])\n",
    "    #train_input_df['L1_idx'] = train_input_df.index % idx_p\n",
    "\n",
    "    # label index for label\n",
    "    train_label_df = pd.DataFrame(train_label, columns = ['L1','L2'])\n",
    "    train_label_df['L1_idx'] = train_label_df.index % idx_p\n",
    "    \n",
    "    # split training and testing data\n",
    "    test_input_df, test_label_df = train_input_df.iloc[75* 80 + 1:, :], train_label_df.iloc[75* 80 + 1:, :]\n",
    "    train_input_df, train_label_df = train_input_df.iloc[:75* 80 + 1, :], train_label_df.iloc[:75* 80 + 1, :]\n",
    "\n",
    "    # group by \n",
    "    #mapping = train_label_df.loc[begin:, :].groupby(by='L1_idx').mean().reset_index().loc[:, ['L1', 'L2', 'L1_idx']] \n",
    "    \n",
    "    #train_input_df = pd.merge(train_input_df, mapping, how='left', on='L1_idx')\n",
    "\n",
    "    #train_input_df = pd.get_dummies(train_input_df, prefix=['L'], columns=['L1_idx'])\n",
    "\n",
    "    train_input_df = train_input_df.loc[begin:, :]\n",
    "\n",
    "\n",
    "    #print(train_input_df.head())\n",
    "    \n",
    "    # testing data\n",
    "    # group by\n",
    "    #test_input_df = test_input_df.merge(mapping, how = 'left', on='L1_idx')\n",
    "\n",
    "    #test_input_df = pd.get_dummies(test_input_df, prefix=['L'], columns=['L1_idx'])\n",
    "\n",
    "    #print(test_input_df.head())\n",
    "\n",
    "    train_input = train_input_df.to_numpy()\n",
    "    test_input = test_input_df.to_numpy()\n",
    "    \n",
    "    train_label = train_label_df.drop(['L1_idx'], axis=1).to_numpy()\n",
    "    test_label = test_label_df.drop(['L1_idx'], axis=1).to_numpy()\n",
    "    \n",
    "    #(train_input.shape)\n",
    "    #print(test_input.shape)\n",
    "    \n",
    "    \n",
    "    return train_input, train_label, test_input, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf8e6108-810c-4eaa-a947-72bef20d573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "       # neuron_params = {\n",
    "       #         'threshold'     : 0.1,\n",
    "       #         'current_decay' : 1,\n",
    "       #         'voltage_decay' : 0.1,\n",
    "       #         'requires_grad' : True,     \n",
    "       #     }\n",
    "        #neuron_params_drop = {**neuron_params, 'dropout' : slayer.neuron.Dropout(p=0.05),}\n",
    "        neuron_params = {\n",
    "                'threshold'     : 2.0,\n",
    "                'current_decay' : 0.25,\n",
    "                'voltage_decay' : 0.03,\n",
    "                'tau_grad'      : 0.03,\n",
    "                'scale_grad'    : 3,\n",
    "                'requires_grad' : True,   \n",
    "            \n",
    "            }\n",
    "        \n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "                #slayer.block.cuba.Input(neuron_params),\n",
    "                #slayer.block.cuba.Recurrent(neuron_params, input_size, 8, weight_norm=True, delay=True),\n",
    "                slayer.block.cuba.Dense(neuron_params, input_size, 8, weight_norm=True, delay=True, weight_scale=1.0),\n",
    "                slayer.block.cuba.Dense(neuron_params, 8, 1, weight_norm=True, delay=True),\n",
    "                #slayer.block.cuba.Dense(neuron_params, 8, 1, weight_norm=True, delay=True),\n",
    "                #slayer.block.cuba.Dense(neuron_params, 128, output_size, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, input_size, 64, weight_scale=2, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, 64, 128, weight_scale=2, weight_norm=True),\n",
    "                #slayer.block.sigma_delta.Dense(sdnn_dense_params, 128, output_size, weight_scale=2, weight_norm=True)\n",
    "                #slayer.block.cuba.Recurrent(cuba_params, 100, 50),\n",
    "                #slayer.block.cuba.KWTA(cuba_params, 50, 50, num_winners=5)\n",
    "            ])\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        for block in self.blocks:\n",
    "            spike = block(spike)\n",
    "        return spike\n",
    "\n",
    "    def export_hdf5(self, filename):\n",
    "        # network export to hdf5 format\n",
    "        h = h5py.File(filename, 'w')\n",
    "        layer = h.create_group('layer')\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            b.export_hdf5(layer.create_group(f'{i}'))\n",
    "\n",
    "class DNNNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2)\n",
    "        #self.fc2 = nn.Linear(16, 2)\n",
    "        #self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self,x, x1, x2):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = torch.cat((x, x2), axis=1)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.act(x)\n",
    "        #x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=1, num_layers=1, batch_first=True, bias=False)\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=32, num_layers=1, batch_first=True, bias=False)\n",
    "        #self.rnn.weight_ih_l0.requires_grad_(False)\n",
    "        #self.rnn.weight_hh_l0.requires_grad_(False)\n",
    "        #self.fc1 = nn.Linear(16, 8)\n",
    "        self.fc1 = nn.Linear(32, output_size)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_x, _ = self.rnn(x)\n",
    "        rnn_x = rnn_x[:, -1, :]\n",
    "        #x = x.flatten(start_dim=1)\n",
    "        #x = torch.cat((x, rnn_x), axis=1)\n",
    "        x = self.fc1(rnn_x)\n",
    "        # x = self.act(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.act(x)\n",
    "        # x = self.fc3(x)\n",
    "        return x, rnn_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99faab4-fe2e-42c0-b1ab-ad1f8b4a3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, net_snn, net_dnn, rc, num_frame = 19):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target.cpu().clone().detach().numpy())\n",
    "        #target = torch.tensor(target).float()\n",
    "        #input = torch.tensor(input).float()\n",
    "        #input1 = torch.tensor(input1).float()\n",
    "        #input2 = torch.tensor(input2).float\n",
    "        \n",
    "        target = target.float().to(device)\n",
    "        input = input.float().to(device)\n",
    "        input1 = input1.float().to(device)\n",
    "        input2 = input2.float().to(device)\n",
    "    \n",
    "        output = net_snn(input)\n",
    "        \n",
    "        output = net_dnn(output, input1, input2).cpu().clone().detach().numpy()\n",
    "        all_output.append(output)\n",
    "        \n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "   \n",
    "    predict_time = rc.time_to_freq(all_output, num_frame, remove_delay=False)\n",
    "    target_time = rc.time_to_freq(labels, num_frame, remove_delay=False)\n",
    "    return rc.my_new_test(predict_time, target_time)\n",
    "\n",
    "\n",
    "def test_rnn(test_loader, model, rc, nb_inputs, num_frame=19):\n",
    "    all_output = []\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for input, input1, input2, target in test_loader:\n",
    "        inputs.append(input)\n",
    "        labels.append(target.cpu().detach().numpy())\n",
    "        target = target.float().to(device)\n",
    "        input = input.float().to(device)\n",
    "        input1 = input1.float().to(device)\n",
    "        input2 = input2.float().to(device)\n",
    "        \n",
    "        input2 = input2.view(input2.shape[0], nb_inputs, 4)\n",
    "        \n",
    "        output, med = model(input2)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        all_output.append(output)\n",
    "        \n",
    "    \n",
    "    all_output = np.concatenate(all_output, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "   \n",
    "    predict_time = rc.time_to_freq(all_output, num_frame, remove_delay=False)\n",
    "    target_time = rc.time_to_freq(labels, num_frame, remove_delay=False)\n",
    "    return rc.my_new_test(predict_time, target_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d365927f-38ef-4ec0-bc13-068701186303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_loss(teacher_output, student_output, alpha, target, mode=True):\n",
    "    # student loss 1\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    \n",
    "    #compare \n",
    "    loss_t = loss_fn(teacher_output, target)\n",
    "    loss_s = loss_fn(student_output, target)\n",
    "    \n",
    "    loss1 = loss_fn(student_output, target)\n",
    "    loss2 = loss_fn(student_output, teacher_output)\n",
    "    if mode:\n",
    "        if loss_t >= loss_s:\n",
    "            return loss1\n",
    "        else:\n",
    "            return loss1 + alpha * loss2 #loss1 + alpha * loss2\n",
    "    else:\n",
    "        return loss1 + alpha * loss2\n",
    "    \n",
    "    \n",
    "def mutual_loss1(teacher_output, student_output, alpha, target, mode=True):\n",
    "    # student loss 1\n",
    "    loss_fn = torch.nn.L1Loss(reduction='none')\n",
    "    \n",
    "    #compare \n",
    "    loss_t = loss_fn(teacher_output, target)\n",
    "    loss_s = loss_fn(student_output, target)\n",
    "    \n",
    "    A_p = loss_t >= loss_s\n",
    "    A_n = ~A_p\n",
    "    \n",
    "    loss1 = loss_fn(student_output, target)\n",
    "    loss2 = loss_fn(student_output, teacher_output)\n",
    "    \n",
    "    # print(loss1.shape, loss2.shape, A_p)\n",
    "    \n",
    "    loss = A_p * loss1 + A_n * (loss1 + alpha * loss2)\n",
    "    \n",
    "    return torch.sum(loss)\n",
    "\n",
    "def med_loss1(student_output, teacher_output, med_student, med_teacher, target, alpha=0.05):\n",
    "    med_loss_fn = torch.nn.L1Loss(reduction='none')\n",
    "    \n",
    "    #compare \n",
    "    loss_t = loss_fn(teacher_output, target)\n",
    "    loss_s = loss_fn(student_output, target)\n",
    "    \n",
    "    A_p = loss_t < loss_s\n",
    "    \n",
    "    loss = A_p * alpha * med_loss_fn(med_student, med_teacher)\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def mutual_train_med(trainloader, testloader, net_snn, net_dnn, net_teacher, rc, nb_inputs, lr=2e-3, nb_epochs=10):\n",
    "    params = list(net_snn.parameters()) + list(net_dnn.parameters())\n",
    "    optimizer_student = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999))\n",
    "    optimizer_teacher = torch.optim.Adam(net_teacher.parameters(), lr=lr, betas=(0.9,0.999), weight_decay=1e-4)\n",
    "    scheduler_student = torch.optim.lr_scheduler.StepLR(optimizer_student, step_size=100, gamma=0.1)\n",
    "    scheduler_teacher = torch.optim.lr_scheduler.StepLR(optimizer_teacher, step_size=100, gamma=0.1)\n",
    "    \n",
    "    #loss_fn = slayer.loss.SpikeTime(time_constant=2, filter_order=2, reduction='mean').to(device)\n",
    "    #loss_fn = torch.nn.SmoothL1Loss()\n",
    "    loss_hist = []\n",
    "    min_ber = 10000000000000000.0\n",
    "    \n",
    "    spike_med_size = 100\n",
    "    rc_med_size = 32\n",
    "    \n",
    "    med_loss_fn = torch.nn.L1Loss()\n",
    "    med_mapping = torch.rand(spike_med_size, rc_med_size, requires_grad = False).to(device)\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        loss_hist = []\n",
    "        for x1_local, x2_local, x3_local, y_local in trainloader:\n",
    "            x1_local = x1_local.float().to(device)\n",
    "            x2_local = x2_local.float().to(device)\n",
    "            x3_local = x3_local.float().to(device)\n",
    "            y_local = y_local.float().to(device)\n",
    "    \n",
    "            optimizer_student.zero_grad()\n",
    "            optimizer_teacher.zero_grad()\n",
    "            \n",
    "            # snn model \n",
    "            output_spike_med = net_snn(x1_local)\n",
    "            output = net_dnn(output_spike_med, x2_local, x3_local)\n",
    "            \n",
    "            # lstm model\n",
    "            x3_local = x3_local.view(x3_local.shape[0], nb_inputs, 4)\n",
    "            teacher_output, med = net_teacher(x3_local)\n",
    "            \n",
    "            student_loss = mutual_loss1(teacher_output.clone().detach(), output, alpha=0.1, target=y_local, mode=True)\n",
    "            teacher_loss = mutual_loss1(output.clone().detach(), teacher_output, alpha=0.05, target=y_local, mode=True)\n",
    "           \n",
    "            output_spike_med_flatten = torch.flatten(output_spike_med, start_dim=1)\n",
    "            map_spike_med = torch.matmul(output_spike_med_flatten, med_mapping) \n",
    "            med_loss = med_loss_fn(map_spike_med, med.clone().detach())\n",
    "            \n",
    "            student_loss += 0.1 * med_loss\n",
    "            \n",
    "            student_loss.backward()\n",
    "            teacher_loss.backward()\n",
    "            \n",
    "            optimizer_student.step()\n",
    "            optimizer_teacher.step()\n",
    "            loss_hist.append(student_loss.item() + teacher_loss.item())\n",
    "        \n",
    "        test_ber = test(test_loader, net_snn, net_dnn, rc, num_frame=19)\n",
    "        test_rnn(test_loader, net_teacher, rc, nb_inputs, num_frame=19)\n",
    "        min_ber = min(min_ber, test_ber)\n",
    "    return min_ber\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "663fb05d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "0.11787280701754387\n",
      "18\n",
      "0.02631578947368421\n",
      "19\n",
      "0.021929824561403508\n",
      "20\n",
      "0.020285087719298246\n",
      "ML SNN:  0.046600877192982455\n",
      "RC:  0.0\n",
      "KD SNN:  0.0\n"
     ]
    }
   ],
   "source": [
    "silent = True\n",
    "method = 'RLS'  # RLS; INV; INV+RLS\n",
    "# N_total_frame = 17\n",
    "N_total_frame = 94\n",
    "N_sync_frame = 4\n",
    "# SNR_list = np.arange(1,20,2)\n",
    "SNR_list = [15]\n",
    "\n",
    "# Dataset selection\n",
    "folder_name = 'data/S2/'  # LOS_Near:S2, LOS_Far:S3, NLOS:S1\n",
    "output_folder = 'data_outputs/S1'\n",
    "\n",
    "if folder_name == 'data/S1/':  # NLOS\n",
    "    delay = 0\n",
    "    packet_num = 21\n",
    "elif folder_name == 'data/S2/':  # LOS_Near\n",
    "    delay = 1\n",
    "    packet_num = 27 # correct\n",
    "elif folder_name == 'data/S3/':  # LOS_Far\n",
    "    delay = 1\n",
    "    packet_num = 22 # 23\n",
    "else:\n",
    "    print(\"Undefined Dataset\")\n",
    "    exit(1)\n",
    "    \n",
    "window_size = 2\n",
    "N_reservoir = 16\n",
    "debug = False\n",
    "\n",
    "ber_record = []\n",
    "dfe_ber_record = []\n",
    "LS_ber_record = []\n",
    "comb_ber_record = []\n",
    "sta_ber_record = []\n",
    "tanh_lut = tanh(\n",
    "    input_bit=8,\n",
    "    dx_bit=8,\n",
    "    slope_fmt=(10, 10),\n",
    "    intercept_fmt=(19, 19),\n",
    "    max=8,\n",
    "    better_lut=True,\n",
    "    verbose=False,\n",
    "    plot=False)\n",
    "\n",
    "SNR = SNR_list[0]\n",
    "\n",
    "overall_ber = 0.0\n",
    "overall_teacher_ber = 0.0\n",
    "overall_KD_ber = 0.0\n",
    "cnt = 0\n",
    "\n",
    "# except 2\n",
    "packets = [17, 18, 19, 20]  # [11, 13, 14, 16,]\n",
    "for i in packets:\n",
    "    print(i)\n",
    "    rc = RC(silent, method, N_total_frame, N_sync_frame, SNR, delay, window_size, i,\n",
    "            N_reservoir=16,\n",
    "            spectral_radius=0.2,\n",
    "            sparsity=0.4,\n",
    "            noise=1e-6,\n",
    "            lut_activation=False,  # True,\n",
    "            tanh_lut=tanh_lut,\n",
    "            input_scale=25,  #40, #50, # 25,\n",
    "            reservoir_input_scale = 8,  #4,  #5,\n",
    "            show_wout=False,\n",
    "            output_folder= output_folder,\n",
    "            debug=debug,\n",
    "            use_fpga= None,\n",
    "            sock= None,  # usock\n",
    "            addr = None) # addr\n",
    "\n",
    "    train_input, train_label, test_input, test_label = rc.run()\n",
    "    RC_test_input = np.load('gt_test_input_1.npy')\n",
    "    RC_train_input = np.load('gt_train_input_1.npy')\n",
    "    RC_train_label = np.load('gt_train_label_1.npy')\n",
    " \n",
    "    #print(RC_test_input.shape, test_input.shape)\n",
    "    #print(\"test_input_diff: \", torch.nn.MSELoss()(torch.tensor(RC_test_input), torch.tensor(test_input)))\n",
    "\n",
    "    #print(\"train_input_diff: \", torch.nn.MSELoss()(torch.tensor(RC_train_input), torch.tensor(train_input)))\n",
    "\n",
    "    #print(\"train_label_diff: \", torch.nn.MSELoss()(torch.tensor(RC_train_label), torch.tensor(train_label)))\n",
    "\n",
    "    train_mean = np.mean(train_input)\n",
    "    train_std = np.std(train_input)\n",
    "\n",
    "    train_input = (train_input - train_mean) / train_std\n",
    "    test_input = (test_input - train_mean) / train_std\n",
    "    train_label = 1.0 * train_label\n",
    "    \n",
    "    train_input, train_label, test_input, test_label = pre_processing(train_input, train_label)\n",
    "    \n",
    "    \n",
    "    nb_inputs  = 6\n",
    "    nb_steps  = 100\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    import scipy.io\n",
    "\n",
    "    from dataset import Dataset, RegTorchSeasonalitySpikingDataset, RegSpikingDataset, RegTorchSpikingDataset, RegTorchSeasonalityLinearSpikingDataset\n",
    "    train_data = RegTorchSeasonalityLinearSpikingDataset(train_input, train_label, nb_inputs, nb_steps)\n",
    "    test_data = RegTorchSeasonalityLinearSpikingDataset(test_input, test_label, nb_inputs, nb_steps)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "\n",
    "    # KD learning\n",
    "    #net_snn = Network(nb_inputs * 4, 2).to(device)\n",
    "    #net_dnn = DNNNetwork(1 * nb_steps + nb_inputs * 4 + 0 + 0 * nb_inputs, 2).to(device)\n",
    "    #net_rc = TOriFloatDFRSystem(n_hidden=16, n_fc=64)\n",
    "    \n",
    "    #teacher_min_ber = train_dnn(train_loader, test_loader, net_rc, rc, nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    #KD_min_ber = KD_train(train_loader, test_loader, net_snn, net_dnn, net_rc, rc, './teacher.pt', nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    \n",
    "    # mutual learning\n",
    "    net_snn = Network(nb_inputs * 4, 2).to(device)\n",
    "    net_dnn = DNNNetwork(1 * nb_steps + nb_inputs * 4 + 0 + 0 * nb_inputs, 2).to(device)\n",
    "    net_lstm = LSTMNet(4, 2).to(device)\n",
    "    min_ber = mutual_train_med(train_loader, test_loader, net_snn, net_dnn, net_lstm, rc, nb_inputs, lr=1e-3, nb_epochs=100)\n",
    "    \n",
    "    overall_ber += min_ber\n",
    "    print(min_ber)\n",
    "    #overall_KD_ber += KD_min_ber\n",
    "    #overall_teacher_ber += teacher_min_ber\n",
    "    cnt += 1\n",
    "    #print(i, min_ber, cnt, overall_ber)\n",
    "\n",
    "print(\"ML SNN: \", overall_ber / len(packets))\n",
    "print(\"RC: \", overall_teacher_ber / len(packets))\n",
    "print(\"KD SNN: \", overall_KD_ber / len(packets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c576ab1-4f28-4b2a-bb31-d2a84b3869bc",
   "metadata": {},
   "source": [
    "11\n",
    "0.10361842105263158\n",
    "13\n",
    "0.07236842105263158\n",
    "14\n",
    "0.06304824561403509\n",
    "16\n",
    "0.0893640350877193\n",
    "17\n",
    "0.11787280701754387\n",
    "18\n",
    "0.02631578947368421\n",
    "19\n",
    "0.021929824561403508\n",
    "20\n",
    "0.020285087719298246\n",
    "\n",
    "best=0.06435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3cfb9b0-ad20-46a0-8234-5205635b204d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09525767543859649"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.09525767543859649\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f196a56d-a839-4971-9c79-7ed11d2e435b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08703399122807018"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.08703399122807018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed2c5e-b402-4992-9ca1-b3cd79854bbb",
   "metadata": {},
   "source": [
    "SNR 15dB\n",
    "ML SNN:  0.06325383771929824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8f188-a209-47ae-92ff-154ba57d6378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l:Python",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
